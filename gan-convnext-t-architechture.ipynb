{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-03T15:04:56.546696Z",
     "iopub.status.busy": "2025-10-03T15:04:56.546365Z",
     "iopub.status.idle": "2025-10-03T15:05:03.048234Z",
     "shell.execute_reply": "2025-10-03T15:05:03.047522Z",
     "shell.execute_reply.started": "2025-10-03T15:04:56.546665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T11:58:21.227964Z",
     "iopub.status.busy": "2025-10-10T11:58:21.227190Z",
     "iopub.status.idle": "2025-10-10T11:58:21.231734Z",
     "shell.execute_reply": "2025-10-10T11:58:21.230965Z",
     "shell.execute_reply.started": "2025-10-10T11:58:21.227937Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (2.20.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: keras>=3.10.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: namex in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: rich in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: pillow in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard~=2.20.0->tensorflow) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.20.0->tensorflow) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.10.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # 0 = all logs, 1 = filter INFO, 2 = filter WARNING, 3 = filter ERROR\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.3.3-cp39-cp39-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.8 MB 16.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.9.4-cp39-cp39-macosx_11_0_arm64.whl (7.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8 MB 14.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-macosx_13_0_arm64.whl (37.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 37.9 MB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorflow in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (2.0.2)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.3 MB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[K     |████████████████████████████████| 509 kB 15.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[K     |████████████████████████████████| 347 kB 20.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from matplotlib) (25.0)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 7.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "\u001b[K     |████████████████████████████████| 113 kB 12.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=8 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from matplotlib) (11.3.0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.60.1-cp39-cp39-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (249 kB)\n",
      "\u001b[K     |████████████████████████████████| 249 kB 19.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=3.6.6 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: optree in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: namex in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: rich in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard~=2.20.0->tensorflow) (8.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.10.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/srabontideb/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Installing collected packages: tzdata, threadpoolctl, scipy, pytz, pyparsing, kiwisolver, joblib, importlib-resources, fonttools, cycler, contourpy, scikit-learn, pandas, opencv-python, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.60.1 importlib-resources-6.5.2 joblib-1.5.2 kiwisolver-1.4.7 matplotlib-3.9.4 opencv-python-4.12.0.88 pandas-2.3.3 pyparsing-3.2.5 pytz-2025.2 scikit-learn-1.6.1 scipy-1.13.1 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn pandas matplotlib opencv-python tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T11:58:24.972334Z",
     "iopub.status.busy": "2025-10-10T11:58:24.972063Z",
     "iopub.status.idle": "2025-10-10T11:58:25.874742Z",
     "shell.execute_reply": "2025-10-10T11:58:25.874078Z",
     "shell.execute_reply.started": "2025-10-10T11:58:24.972314Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 8000; Classes: ['00', '01', '02', '03', '04', '05', '06', '07']\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data_dir = \"/Users/srabontideb/Downloads/Young Learner_s Research Lab/GAN/kvasir_bilateral_filtered\"\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Get class folders\n",
    "class_names = sorted(os.listdir(data_dir))\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
    "\n",
    "for cls_name in class_names:\n",
    "    cls_folder = os.path.join(data_dir, cls_name)\n",
    "    for img_path in glob.glob(os.path.join(cls_folder, '*.*')):\n",
    "        image_paths.append(img_path)\n",
    "        labels.append(class_to_idx[cls_name])\n",
    "\n",
    "image_paths, labels = shuffle(image_paths, labels, random_state=42)\n",
    "X = np.array(image_paths)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"Total images: {len(X)}; Classes: {class_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T11:58:29.856758Z",
     "iopub.status.busy": "2025-10-10T11:58:29.856052Z",
     "iopub.status.idle": "2025-10-10T11:58:30.050810Z",
     "shell.execute_reply": "2025-10-10T11:58:30.050033Z",
     "shell.execute_reply.started": "2025-10-10T11:58:29.856734Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train+Val: 6400   Test: 1600\n",
      "Fold 1: train=5120, val=1280\n",
      "Fold 2: train=5120, val=1280\n",
      "Fold 3: train=5120, val=1280\n",
      "Fold 4: train=5120, val=1280\n",
      "Fold 5: train=5120, val=1280\n",
      "Saved splits to: /Users/srabontideb/Downloads/Young Learner_s Research Lab/GAN/kvasir_bilateral_filtered/output/splits_20251013_1039\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 1) Train/Val/Test split (stratified)\n",
    "# ============================\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np, pandas as pd, time, os\n",
    "from pathlib import Path\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE    = 0.20      # <- change if you want (e.g., 0.1 or 0.15)\n",
    "N_SPLITS     = 5         # k for CV\n",
    "\n",
    "# X = np.array(image_paths); y = np.array(labels)  # <- you already have these\n",
    "\n",
    "# Hold out the test set FIRST (never touched during CV/early stopping)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Safety checks (no overlap)\n",
    "assert set(X_trainval).isdisjoint(set(X_test)), \"Leak: train/val and test overlap!\"\n",
    "assert len(X_trainval) + len(X_test) == len(X)\n",
    "\n",
    "print(f\"Train+Val: {len(X_trainval)}   Test: {len(X_test)}\")\n",
    "\n",
    "# ============================\n",
    "# 2) Stratified K-Fold on TRAIN+VAL ONLY\n",
    "# ============================\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "folds_data = []\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_trainval, y_trainval), start=1):\n",
    "    folds_data.append({\n",
    "        \"fold\": fold,\n",
    "        \"train_paths\": X_trainval[tr_idx],\n",
    "        \"train_labels\": y_trainval[tr_idx],\n",
    "        \"val_paths\":   X_trainval[val_idx],\n",
    "        \"val_labels\":  y_trainval[val_idx],\n",
    "    })\n",
    "    print(f\"Fold {fold}: train={len(tr_idx)}, val={len(val_idx)}\")\n",
    "\n",
    "# ============================\n",
    "# 3) Save splits for reuse (Quick Save will capture these)\n",
    "# ============================\n",
    "OUT = Path(\"/Users/srabontideb/Downloads/Young Learner_s Research Lab/GAN/kvasir_bilateral_filtered/output\")/f\"splits_{time.strftime('%Y%m%d_%H%M')}\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Test set (held-out)\n",
    "pd.DataFrame({\"filepath\": X_test, \"label\": y_test}).to_csv(OUT/\"test.csv\", index=False)\n",
    "\n",
    "# Train/Val folds\n",
    "pd.DataFrame({\"filepath\": X_trainval, \"label\": y_trainval}).to_csv(OUT/\"trainval.csv\", index=False)\n",
    "for f in folds_data:\n",
    "    fold = f[\"fold\"]\n",
    "    pd.DataFrame({\"filepath\": f[\"train_paths\"], \"label\": f[\"train_labels\"]}).to_csv(OUT/f\"train_fold{fold}.csv\", index=False)\n",
    "    pd.DataFrame({\"filepath\": f[\"val_paths\"],   \"label\": f[\"val_labels\"]}).to_csv(OUT/f\"val_fold{fold}.csv\",   index=False)\n",
    "\n",
    "print(\"Saved splits to:\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T11:58:34.657788Z",
     "iopub.status.busy": "2025-10-10T11:58:34.657251Z",
     "iopub.status.idle": "2025-10-10T11:58:35.375840Z",
     "shell.execute_reply": "2025-10-10T11:58:35.374990Z",
     "shell.execute_reply.started": "2025-10-10T11:58:34.657766Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T11:58:38.322794Z",
     "iopub.status.busy": "2025-10-10T11:58:38.322232Z",
     "iopub.status.idle": "2025-10-10T11:58:38.328097Z",
     "shell.execute_reply": "2025-10-10T11:58:38.327318Z",
     "shell.execute_reply.started": "2025-10-10T11:58:38.322773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "IMG_SIZE = (224, 224)  # adjust as needed\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def decode_img(file_path, label):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)  # or decode_png if png\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img, label\n",
    "\n",
    "def make_tf_dataset(file_paths, labels, batch_size=BATCH_SIZE, shuffle_data=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    ds = ds.map(decode_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle_data:\n",
    "        ds = ds.shuffle(buffer_size=len(file_paths))\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T11:58:41.273529Z",
     "iopub.status.busy": "2025-10-10T11:58:41.273258Z",
     "iopub.status.idle": "2025-10-10T11:58:41.335333Z",
     "shell.execute_reply": "2025-10-10T11:58:41.334572Z",
     "shell.execute_reply.started": "2025-10-10T11:58:41.273508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T12:32:53.749255Z",
     "iopub.status.busy": "2025-10-09T12:32:53.748687Z",
     "iopub.status.idle": "2025-10-09T14:00:56.183990Z",
     "shell.execute_reply": "2025-10-09T14:00:56.183344Z",
     "shell.execute_reply.started": "2025-10-09T12:32:53.749233Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= FOLD 1/5 =================\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/convnext/convnext_tiny_notop.h5\n",
      "\u001b[1m111650432/111650432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 0us/step\n",
      "\n",
      "---- Warmup (frozen backbone) ----\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760330391.633763 1930040 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 31/160\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12:09\u001b[0m 6s/step - acc: 0.2163 - loss: 2.5576"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 138\u001b[0m\n\u001b[1;32m    135\u001b[0m cb \u001b[38;5;241m=\u001b[39m get_callbacks(fold_id)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---- Warmup (frozen backbone) ----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m history_warmup \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS_WARMUP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# 5.2 Fine-tune: unfreeze backbone\u001b[39;00m\n\u001b[1;32m    147\u001b[0m model\u001b[38;5;241m.\u001b[39mget_layer(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# backbone is the 2nd layer (index 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    219\u001b[0m     ):\n\u001b[0;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1703\u001b[0m   )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) Imports\n",
    "# =========================\n",
    "import os, math, numpy as np, tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses, metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "# Optional: mixed precision for speed on P100 (usually beneficial)\n",
    "set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# Keras ConvNeXt (requires TF/Keras >= 2.11)\n",
    "from tensorflow.keras.applications import ConvNeXtTiny\n",
    "from tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\n",
    "\n",
    "# =========================\n",
    "# 1) Config\n",
    "# =========================\n",
    "IMG_SIZE = (224, 224)         # ConvNeXt default\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_WARMUP = 3             # freeze backbone first\n",
    "EPOCHS_FINETUNE = 12          # then unfreeze\n",
    "LR_WARMUP = 1e-3\n",
    "LR_FINETUNE = 5e-4\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "N_FOLDS = len(folds_data)     # should be 5 for your setup\n",
    "N_CLASSES = len(class_names)\n",
    "\n",
    "# =========================\n",
    "# 2) Data pipeline\n",
    "#    (ConvNeXt expects its own preprocessing; we keep images in [0..255] float and apply convnext_preprocess)\n",
    "# =========================\n",
    "def decode_img_for_convnext(file_path, label):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    # change decode_* if your files are PNG\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    # keep as float32 in [0..255] for convnext_preprocess\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    return img, label\n",
    "\n",
    "# Light on-the-fly augmentation (you can tune as needed)\n",
    "data_augment = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomContrast(0.1),\n",
    "], name=\"augment\")\n",
    "\n",
    "@tf.function\n",
    "def apply_preprocess(x, y):\n",
    "    # ConvNeXt preprocess: channel-wise normalize to ImageNet statistics internally\n",
    "    x = convnext_preprocess(x)   # returns float32\n",
    "    return x, y\n",
    "\n",
    "def make_tf_dataset_convnext(file_paths, labels, training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    ds = ds.shuffle(len(file_paths), reshuffle_each_iteration=True) if training else ds\n",
    "    ds = ds.map(decode_img_for_convnext, num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.map(lambda x, y: (data_augment(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.map(apply_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# =========================\n",
    "# 3) ConvNeXt-Tiny model builder\n",
    "# =========================\n",
    "def build_convnext_tiny(num_classes=N_CLASSES, image_size=IMG_SIZE, trainable_backbone=False):\n",
    "    # Backbone\n",
    "    backbone = ConvNeXtTiny(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(image_size[0], image_size[1], 3),\n",
    "        pooling=None\n",
    "    )\n",
    "    backbone.trainable = trainable_backbone  # warmup: False; finetune: True\n",
    "\n",
    "    inputs = layers.Input(shape=(image_size[0], image_size[1], 3))\n",
    "    # NOTE: we already applied convnext_preprocess in the dataset, so we pass inputs directly.\n",
    "    x = backbone(inputs, training=False)\n",
    "\n",
    "    # Head (recommend GAP + BN + dropout + Dense)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    # Final classifier\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)  # force float32 for numeric stability\n",
    "\n",
    "    model = models.Model(inputs, outputs, name=\"ConvNeXtTiny_Kvasir\")\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# 4) Training utilities\n",
    "# =========================\n",
    "def compile_model(model, lr):\n",
    "    opt = optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    "    )\n",
    "\n",
    "def get_callbacks(fold_id):\n",
    "    os.makedirs(\"convnext_runs\", exist_ok=True)\n",
    "    return [\n",
    "        EarlyStopping(monitor=\"val_acc\", patience=6, mode=\"max\", restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.5, patience=3, verbose=1, min_lr=1e-6),\n",
    "        ModelCheckpoint(\n",
    "            filepath=f\"convnext_runs/best_convnextT_fold{fold_id}.h5\",\n",
    "            monitor=\"val_acc\", mode=\"max\", save_best_only=True, verbose=1\n",
    "        ),\n",
    "        CSVLogger(f\"convnext_runs/training_log_fold{fold_id}.csv\", append=False)\n",
    "    ]\n",
    "\n",
    "# =========================\n",
    "# 5) 5-Fold training (warmup + finetune)\n",
    "# =========================\n",
    "fold_metrics = []\n",
    "\n",
    "for fold_id in range(N_FOLDS):\n",
    "    print(f\"\\n================= FOLD {fold_id+1}/{N_FOLDS} =================\")\n",
    "\n",
    "    # Build datasets\n",
    "    train_paths = folds_data[fold_id]['train_paths']\n",
    "    train_labels = folds_data[fold_id]['train_labels']\n",
    "    val_paths   = folds_data[fold_id]['val_paths']\n",
    "    val_labels  = folds_data[fold_id]['val_labels']\n",
    "\n",
    "    train_ds = make_tf_dataset_convnext(train_paths, train_labels, training=True)\n",
    "    val_ds   = make_tf_dataset_convnext(val_paths,   val_labels,   training=False)\n",
    "\n",
    "    # 5.1 Warmup: freeze backbone\n",
    "    model = build_convnext_tiny(trainable_backbone=False)\n",
    "    compile_model(model, lr=LR_WARMUP)\n",
    "    cb = get_callbacks(fold_id)\n",
    "\n",
    "    print(\"\\n---- Warmup (frozen backbone) ----\")\n",
    "    history_warmup = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS_WARMUP,\n",
    "        callbacks=cb,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 5.2 Fine-tune: unfreeze backbone\n",
    "    model.get_layer(index=1).trainable = True  # backbone is the 2nd layer (index 1)\n",
    "    # (Optional) fine-tune last N blocks only:\n",
    "    # for l in model.get_layer(index=1).layers[:-20]:  # keep last ~20 layers trainable\n",
    "    #     l.trainable = False\n",
    "\n",
    "    compile_model(model, lr=LR_FINETUNE)\n",
    "    print(\"\\n---- Fine-tuning (unfrozen backbone) ----\")\n",
    "    history_ft = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS_FINETUNE,\n",
    "        callbacks=cb,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate best weights\n",
    "    eval_res = model.evaluate(val_ds, verbose=0)\n",
    "    # eval_res -> [loss, acc]\n",
    "    fold_metrics.append({\"fold\": fold_id+1, \"val_loss\": float(eval_res[0]), \"val_acc\": float(eval_res[1])})\n",
    "    print(f\"FOLD {fold_id+1} → val_loss: {eval_res[0]:.4f} | val_acc: {eval_res[1]:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# 6) Summary across folds\n",
    "# =========================\n",
    "val_accs  = [m[\"val_acc\"]  for m in fold_metrics]\n",
    "val_losses= [m[\"val_loss\"] for m in fold_metrics]\n",
    "\n",
    "print(\"\\n========= 5-Fold Summary (ConvNeXt-T) =========\")\n",
    "for m in fold_metrics:\n",
    "    print(f\"Fold {m['fold']}:  val_acc={m['val_acc']:.4f},  val_loss={m['val_loss']:.4f}\")\n",
    "\n",
    "print(f\"\\nMean val_acc: {np.mean(val_accs):.4f}  (± {np.std(val_accs):.4f})\")\n",
    "print(f\"Mean val_loss: {np.mean(val_losses):.4f} (± {np.std(val_losses):.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T14:11:34.046841Z",
     "iopub.status.busy": "2025-10-09T14:11:34.046604Z",
     "iopub.status.idle": "2025-10-09T14:11:36.007085Z",
     "shell.execute_reply": "2025-10-09T14:11:36.006422Z",
     "shell.execute_reply.started": "2025-10-09T14:11:34.046825Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] /kaggle/working/convnext_runs/training_log_fold0.csv not found\n",
      "[skip] /kaggle/working/convnext_runs/training_log_fold1.csv not found\n",
      "[skip] /kaggle/working/convnext_runs/training_log_fold2.csv not found\n",
      "[skip] /kaggle/working/convnext_runs/training_log_fold3.csv not found\n",
      "[skip] /kaggle/working/convnext_runs/training_log_fold4.csv not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b1/f_5mk66d2773tzpl944mshqw0000gn/T/ipykernel_28039/1383662929.py:48: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.title(\"Validation Accuracy — All Folds\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Accuracy\"); plt.grid(True, linewidth=0.3); plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b1/f_5mk66d2773tzpl944mshqw0000gn/T/ipykernel_28039/1383662929.py:58: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.title(\"Validation Loss — All Folds\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Loss\"); plt.grid(True, linewidth=0.3); plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inline visualization of training/validation curves from CSVLogger outputs\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "LOG_DIR = Path(\"/kaggle/working/convnext_runs\")\n",
    "N_FOLDS = 5  # change if needed\n",
    "\n",
    "def pick_cols(df):\n",
    "    m = {c.lower().strip(): c for c in df.columns}\n",
    "    acc = m.get(\"acc\") or m.get(\"accuracy\") or m.get(\"sparse_categorical_accuracy\") or m.get(\"categorical_accuracy\")\n",
    "    val_acc = m.get(\"val_acc\") or m.get(\"val_accuracy\") or m.get(\"val_sparse_categorical_accuracy\") or m.get(\"val_categorical_accuracy\")\n",
    "    loss = m.get(\"loss\")\n",
    "    val_loss = m.get(\"val_loss\")\n",
    "    return acc, val_acc, loss, val_loss\n",
    "\n",
    "# --- Per-fold panels ---\n",
    "for f in range(N_FOLDS):\n",
    "    csvp = LOG_DIR / f\"training_log_fold{f}.csv\"\n",
    "    if not csvp.exists():\n",
    "        print(f\"[skip] {csvp} not found\"); \n",
    "        continue\n",
    "    df = pd.read_csv(csvp)\n",
    "    acc, val_acc, loss, val_loss = pick_cols(df)\n",
    "    epochs = np.arange(1, len(df)+1)\n",
    "\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(7,4))\n",
    "    if acc:     plt.plot(epochs, df[acc], label=\"train acc\")\n",
    "    if val_acc: plt.plot(epochs, df[val_acc], label=\"val acc\")\n",
    "    plt.title(f\"Fold {f} • Accuracy\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.grid(True, linewidth=0.3); plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Loss\n",
    "    plt.figure(figsize=(7,4))\n",
    "    if loss:     plt.plot(epochs, df[loss], label=\"train loss\")\n",
    "    if val_loss: plt.plot(epochs, df[val_loss], label=\"val loss\")\n",
    "    plt.title(f\"Fold {f} • Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.grid(True, linewidth=0.3); plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# --- All-folds comparison (validation curves only) ---\n",
    "plt.figure(figsize=(8,5))\n",
    "for f in range(N_FOLDS):\n",
    "    csvp = LOG_DIR / f\"training_log_fold{f}.csv\"\n",
    "    if not csvp.exists(): continue\n",
    "    df = pd.read_csv(csvp)\n",
    "    _, val_acc, _, val_loss = pick_cols(df)\n",
    "    if val_acc: plt.plot(np.arange(1, len(df)+1), df[val_acc], label=f\"Fold {f}\")\n",
    "plt.title(\"Validation Accuracy — All Folds\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Accuracy\"); plt.grid(True, linewidth=0.3); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for f in range(N_FOLDS):\n",
    "    csvp = LOG_DIR / f\"training_log_fold{f}.csv\"\n",
    "    if not csvp.exists(): continue\n",
    "    df = pd.read_csv(csvp)\n",
    "    _, _, _, val_loss = pick_cols(df)\n",
    "    if val_loss: plt.plot(np.arange(1, len(df)+1), df[val_loss], label=f\"Fold {f}\")\n",
    "plt.title(\"Validation Loss — All Folds\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Loss\"); plt.grid(True, linewidth=0.3); plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T14:11:44.460409Z",
     "iopub.status.busy": "2025-10-09T14:11:44.460120Z",
     "iopub.status.idle": "2025-10-09T14:12:01.457964Z",
     "shell.execute_reply": "2025-10-09T14:12:01.457290Z",
     "shell.execute_reply.started": "2025-10-09T14:11:44.460389Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '/kaggle/working/convnext_runs/best_convnextT_fold4.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m models\u001b[38;5;241m.\u001b[39mModel(inp, out)\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWEIGHTS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# --- Pick a real image (or set IMG_PATH yourself) ---\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpick_image\u001b[39m(root):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/h5py/_hl/files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    557\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    558\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    559\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    560\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    561\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    562\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    563\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 564\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/h5py/_hl/files.py:238\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    237\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 238\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    240\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:56\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:57\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/kaggle/working/convnext_runs/best_convnextT_fold4.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# =============== Robust Grad-CAM (manual forward; heatmap only) ===============\n",
    "import os, glob, random, numpy as np, tensorflow as tf, cv2\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ConvNeXtTiny\n",
    "from tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATA_ROOT = \"/kaggle/input/kvasir-final-preprocessed-data/kvasir_bilateral_filtered\"\n",
    "WEIGHTS   = Path(\"/kaggle/working/convnext_runs/best_convnextT_fold4.h5\")  # change fold if you want\n",
    "IMG_SIZE  = (224, 224)\n",
    "N_CLASSES = 8\n",
    "TARGET    = \"pred\"  # \"pred\" or an int class id\n",
    "OUT_DIR   = Path(\"/Users/srabontideb/Downloads/Young Learner_s Research Lab/GAN/kvasir_bilateral_filtered/output\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Build SAME head you trained and load weights ---\n",
    "def build_model(nc=N_CLASSES, image_size=IMG_SIZE):\n",
    "    bb = ConvNeXtTiny(include_top=False, weights=None,\n",
    "                      input_shape=(image_size[0], image_size[1], 3), pooling=None)\n",
    "    inp = layers.Input(shape=(image_size[0], image_size[1], 3))\n",
    "    x = bb(inp, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(nc, activation=\"softmax\", dtype=\"float32\")(x)\n",
    "    return models.Model(inp, out)\n",
    "\n",
    "model = build_model()\n",
    "model.load_weights(str(WEIGHTS))\n",
    "\n",
    "# --- Pick a real image (or set IMG_PATH yourself) ---\n",
    "def pick_image(root):\n",
    "    exts = (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\",\"*.JPEG\")\n",
    "    c = []\n",
    "    for e in exts: c += glob.glob(os.path.join(root, \"**\", e), recursive=True)\n",
    "    assert c, f\"No images under {root}\"\n",
    "    return random.choice(c)\n",
    "\n",
    "IMG_PATH = pick_image(DATA_ROOT)\n",
    "print(\"Using image:\", IMG_PATH)\n",
    "\n",
    "# --- Preprocess ---\n",
    "def load_for_model(fp, size):\n",
    "    img = tf.io.read_file(fp)\n",
    "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, size)\n",
    "    x = tf.cast(img, tf.float32)          # [0..255]\n",
    "    x = convnext_preprocess(x)            # ConvNeXt preprocessing\n",
    "    return tf.expand_dims(x, 0), tf.cast(img, tf.uint8)\n",
    "\n",
    "x, img_uint8 = load_for_model(IMG_PATH, IMG_SIZE)\n",
    "\n",
    "# --- Helpers ---\n",
    "def find_last_4d_layer_name(m):\n",
    "    # walk from end to start to get the deepest 4D (HWC) tensor layer\n",
    "    for lyr in reversed(m.layers):\n",
    "        shp = getattr(lyr, \"output_shape\", None)\n",
    "        if isinstance(shp, tuple) and len(shp) == 4:\n",
    "            return lyr.name\n",
    "    raise ValueError(\"No 4D conv feature layer found for Grad-CAM.\")\n",
    "\n",
    "LAST_NAME = find_last_4d_layer_name(model)\n",
    "\n",
    "def gradcam_manual(m, x, class_idx, last_name, img_size):\n",
    "    \"\"\"\n",
    "    Manual forward through layers inside one GradientTape to avoid Keras-3 multi-output issues.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        t = x\n",
    "        conv_out = None\n",
    "        for lyr in m.layers[1:]:  # skip InputLayer at index 0\n",
    "            t = lyr(t, training=False)\n",
    "            if lyr.name == last_name:\n",
    "                conv_out = t\n",
    "        preds = t                            # (1, C)\n",
    "        target = preds[:, class_idx]         # scalar per sample\n",
    "    assert conv_out is not None, \"Last conv feature not found during forward pass.\"\n",
    "    grads   = tape.gradient(target, conv_out)          # (1, h, w, C)\n",
    "    weights = tf.reduce_mean(grads, axis=(0,1,2))      # (C,)\n",
    "    cam     = tf.tensordot(conv_out[0], weights, axes=1)  # (h, w)\n",
    "    cam     = tf.nn.relu(cam)\n",
    "    cam     = cam / (tf.reduce_max(cam) + 1e-12)\n",
    "    cam     = tf.image.resize(cam[...,None], img_size)[...,0]\n",
    "    return cam.numpy(), preds.numpy()[0]\n",
    "\n",
    "# --- First pass to get predicted class (then CAM for it) ---\n",
    "cam_tmp, probs0 = gradcam_manual(model, x, class_idx=0, last_name=LAST_NAME, img_size=IMG_SIZE)\n",
    "pred_id = int(np.argmax(probs0))\n",
    "class_idx = pred_id if TARGET == \"pred\" else int(TARGET)\n",
    "\n",
    "# --- Final CAM for chosen class ---\n",
    "cam, probs = gradcam_manual(model, x, class_idx=class_idx, last_name=LAST_NAME, img_size=IMG_SIZE)\n",
    "\n",
    "# --- Save grayscale heatmap ---\n",
    "out_path = OUT_DIR / f\"cam_{Path(IMG_PATH).stem}_class{class_idx}.png\"\n",
    "cv2.imwrite(str(out_path), (np.clip(cam,0,1)*255).astype(np.uint8))\n",
    "print(f\"Saved heatmap → {out_path}\")\n",
    "print(\"Predicted class id:\", pred_id, \"| CAM for class id:\", class_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T14:23:00.984667Z",
     "iopub.status.busy": "2025-10-09T14:23:00.984397Z",
     "iopub.status.idle": "2025-10-09T14:23:00.990094Z",
     "shell.execute_reply": "2025-10-09T14:23:00.989376Z",
     "shell.execute_reply.started": "2025-10-09T14:23:00.984646Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['00', '01', '02', '03', '04', '05', '06', '07']\n",
      "Fold: 4\n"
     ]
    }
   ],
   "source": [
    "# Phase 5: PostSegXAI — setup\n",
    "import os, math, numpy as np, tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Save dirs\n",
    "FOLD_ID = 4           # <-- choose the fold you want to inspect\n",
    "OUT_DIR = f\"postsegxai_fold{FOLD_ID}\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Convenience\n",
    "N_CLASSES = len(class_names)\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Fold:\", FOLD_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T14:23:03.852011Z",
     "iopub.status.busy": "2025-10-09T14:23:03.851387Z",
     "iopub.status.idle": "2025-10-09T14:23:05.197731Z",
     "shell.execute_reply": "2025-10-09T14:23:05.196992Z",
     "shell.execute_reply.started": "2025-10-09T14:23:03.851987Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'convnext_runs/best_convnextT_fold4.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Rebuild + load weights from training (works even if .h5 full-model load fails)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m build_convnext_tiny()\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconvnext_runs/best_convnextT_fold\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mFOLD_ID\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel rebuilt + weights loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/h5py/_hl/files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    557\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    558\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    559\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    560\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    561\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    562\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    563\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 564\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/h5py/_hl/files.py:238\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    237\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 238\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    240\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:56\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:57\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'convnext_runs/best_convnextT_fold4.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# If build_convnext_tiny & convnext_preprocess are not in scope, re-import:\n",
    "from tensorflow.keras.applications import ConvNeXtTiny\n",
    "from tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_convnext_tiny(num_classes=N_CLASSES, image_size=IMG_SIZE, trainable_backbone=True):\n",
    "    backbone = ConvNeXtTiny(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(image_size[0], image_size[1], 3),\n",
    "        pooling=None\n",
    "    )\n",
    "    backbone.trainable = trainable_backbone\n",
    "    inputs = layers.Input(shape=(image_size[0], image_size[1], 3))\n",
    "    x = backbone(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
    "    return models.Model(inputs, outputs, name=\"ConvNeXtTiny_Kvasir\")\n",
    "\n",
    "# Rebuild + load weights from training (works even if .h5 full-model load fails)\n",
    "model = build_convnext_tiny()\n",
    "model.load_weights(f\"convnext_runs/best_convnextT_fold{FOLD_ID}.h5\")\n",
    "print(\"Model rebuilt + weights loaded.\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T14:23:13.966740Z",
     "iopub.status.busy": "2025-10-09T14:23:13.966483Z",
     "iopub.status.idle": "2025-10-09T14:23:13.972603Z",
     "shell.execute_reply": "2025-10-09T14:23:13.971861Z",
     "shell.execute_reply.started": "2025-10-09T14:23:13.966720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths for this fold\n",
    "val_paths = folds_data[FOLD_ID]['val_paths']\n",
    "val_labels = folds_data[FOLD_ID]['val_labels']\n",
    "\n",
    "def load_img_for_convnext(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)    # change to decode_png if needed\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(img, tf.float32)                 # [0..255]\n",
    "    return img\n",
    "\n",
    "def predict_softmax(img_raw):\n",
    "    x = convnext_preprocess(img_raw[None, ...])    # preprocess inside\n",
    "    p = model(x, training=False).numpy()[0]        # (C,)\n",
    "    pred = int(np.argmax(p))\n",
    "    conf = float(np.max(p))\n",
    "    # uncertainty via entropy (nats)\n",
    "    entropy = float(-np.sum(p * np.log(p + 1e-12)))\n",
    "    return pred, conf, entropy, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T14:23:16.735948Z",
     "iopub.status.busy": "2025-10-09T14:23:16.735406Z",
     "iopub.status.idle": "2025-10-09T14:23:16.756283Z",
     "shell.execute_reply": "2025-10-09T14:23:16.755568Z",
     "shell.execute_reply.started": "2025-10-09T14:23:16.735925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Find last 4D feature layer automatically (should be 'convnext_tiny')\n",
    "def get_last_4d_layer(m):\n",
    "    for lyr in reversed(m.layers):\n",
    "        try:\n",
    "            shp = lyr.output_shape\n",
    "        except:\n",
    "            continue\n",
    "        if isinstance(shp, tuple) and len(shp) == 4:\n",
    "            return lyr.name\n",
    "    return m.layers[1].name  # fallback\n",
    "\n",
    "LAST_FEAT = get_last_4d_layer(model)\n",
    "print(\"Grad-CAM feature layer:\", LAST_FEAT)\n",
    "\n",
    "@tf.function\n",
    "def _gradcam_core(img_batch, target_idx, feat_model, head_model):\n",
    "    with tf.GradientTape() as tape:\n",
    "        feats = feat_model(img_batch)                       # (B,h,w,c)\n",
    "        tape.watch(feats)\n",
    "        logits = head_model(feats, training=False)          # (B,C)\n",
    "        cls = logits[:, target_idx]\n",
    "    grads = tape.gradient(cls, feats)                       # (B,h,w,c)\n",
    "    w = tf.reduce_mean(grads, axis=(1,2), keepdims=True)    # (B,1,1,c)\n",
    "    cam = tf.nn.relu(tf.reduce_sum(w * feats, axis=-1))     # (B,h,w)\n",
    "    # normalize per-image\n",
    "    cam_min = tf.reduce_min(cam, axis=(1,2), keepdims=True)\n",
    "    cam_max = tf.reduce_max(cam, axis=(1,2), keepdims=True)\n",
    "    cam = (cam - cam_min) / (cam_max - cam_min + 1e-8)\n",
    "    return cam\n",
    "\n",
    "# Split model into feature extractor + head (after LAST_FEAT)\n",
    "def split_feature_and_head(m, last_feat_name=LAST_FEAT):\n",
    "    feat_layer = m.get_layer(last_feat_name)\n",
    "    feat_model = tf.keras.Model(m.input, feat_layer.output)              # input -> conv feat\n",
    "    # build head: from conv feat to logits, reusing layers after feat layer\n",
    "    idx = [i for (i, l) in enumerate(m.layers) if l.name == last_feat_name][0]\n",
    "    x = tf.keras.Input(shape=feat_layer.output_shape[1:])\n",
    "    y = x\n",
    "    for lyr in m.layers[idx+1:]:\n",
    "        y = lyr(y)\n",
    "    head_model = tf.keras.Model(x, y)\n",
    "    return feat_model, head_model\n",
    "\n",
    "feat_model, head_model = split_feature_and_head(model, LAST_FEAT)\n",
    "\n",
    "def gradcam_heatmap(img_raw, target_idx):\n",
    "    x = convnext_preprocess(img_raw[None, ...])\n",
    "    cam_small = _gradcam_core(x, tf.constant(target_idx, tf.int32), feat_model, head_model)[0] # (h,w)\n",
    "    cam = tf.image.resize(cam_small[..., None], IMG_SIZE)[...,0].numpy()\n",
    "    return np.clip(cam, 0, 1)\n",
    "\n",
    "def overlay_heatmap(img_raw, cam, alpha=0.45):\n",
    "    img = (img_raw.numpy() / 255.0)\n",
    "    cmap = plt.get_cmap('jet')\n",
    "    heat = cmap(cam)[..., :3]\n",
    "    out = (1 - alpha) * img + alpha * heat\n",
    "    return np.clip(out, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T14:24:59.679359Z",
     "iopub.status.busy": "2025-10-09T14:24:59.678802Z",
     "iopub.status.idle": "2025-10-09T14:24:59.692295Z",
     "shell.execute_reply": "2025-10-09T14:24:59.691548Z",
     "shell.execute_reply.started": "2025-10-09T14:24:59.679333Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No such layer: global_average_pooling2d_8. Existing layers are: ['input_layer_18', 'convnext_tiny', 'global_average_pooling2d_2', 'batch_normalization_2', 'dropout_2', 'dense_2'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Head model: GAP -> BN -> Dropout -> Dense\u001b[39;00m\n\u001b[1;32m      9\u001b[0m f_in \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39mbackbone\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])  \u001b[38;5;66;03m# (7, 7, 768)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m gap  \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglobal_average_pooling2d_8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m(f_in)\n\u001b[1;32m     11\u001b[0m bn   \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_normalization_8\u001b[39m\u001b[38;5;124m\"\u001b[39m)(gap)\n\u001b[1;32m     12\u001b[0m drop \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout_8\u001b[39m\u001b[38;5;124m\"\u001b[39m)(bn)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/models/model.py:210\u001b[0m, in \u001b[0;36mModel.get_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m name:\n\u001b[1;32m    209\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m layer\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Existing layers are: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(layer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mlayer\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m     )\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvide either a layer name or layer index at `get_layer`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: No such layer: global_average_pooling2d_8. Existing layers are: ['input_layer_18', 'convnext_tiny', 'global_average_pooling2d_2', 'batch_normalization_2', 'dropout_2', 'dense_2']."
     ]
    }
   ],
   "source": [
    "# Backbone (ConvNeXt) as feature extractor\n",
    "backbone = model.get_layer(\"convnext_tiny\")\n",
    "\n",
    "x_in = tf.keras.Input(shape=(224, 224, 3))     # plain input tensor\n",
    "feat_out = backbone(x_in)                      # (None, 7, 7, 768)\n",
    "feat_model = tf.keras.Model(x_in, feat_out, name=\"feat_model\")\n",
    "\n",
    "# Head model: GAP -> BN -> Dropout -> Dense\n",
    "f_in = tf.keras.Input(shape=backbone.output.shape[1:])  # (7, 7, 768)\n",
    "gap  = model.get_layer(\"global_average_pooling2d_8\")(f_in)\n",
    "bn   = model.get_layer(\"batch_normalization_8\")(gap)\n",
    "drop = model.get_layer(\"dropout_8\")(bn)\n",
    "out  = model.get_layer(\"dense_8\")(drop)\n",
    "head_model = tf.keras.Model(f_in, out, name=\"head_model\")\n",
    "\n",
    "print(\"✅ Rebuilt feat_model and head_model with fresh inputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T14:25:03.067937Z",
     "iopub.status.busy": "2025-10-09T14:25:03.067200Z",
     "iopub.status.idle": "2025-10-09T14:25:03.073761Z",
     "shell.execute_reply": "2025-10-09T14:25:03.072933Z",
     "shell.execute_reply.started": "2025-10-09T14:25:03.067905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _gradcam_core(img_batch, target_idx, feat_model, head_model):\n",
    "    with tf.GradientTape() as tape:\n",
    "        feats = feat_model(img_batch, training=False)   # (B, 7, 7, 768)\n",
    "        tape.watch(feats)\n",
    "        logits = head_model(feats, training=False)      # (B, num_classes)\n",
    "        loss = logits[:, target_idx]\n",
    "\n",
    "    grads = tape.gradient(loss, feats)                  # (B,7,7,768)\n",
    "    pooled = tf.reduce_mean(grads, axis=(1,2), keepdims=True)\n",
    "    cam = tf.reduce_sum(feats * pooled, axis=-1)        # (B,7,7)\n",
    "    return cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T14:25:07.920989Z",
     "iopub.status.busy": "2025-10-09T14:25:07.920728Z",
     "iopub.status.idle": "2025-10-09T14:25:07.925487Z",
     "shell.execute_reply": "2025-10-09T14:25:07.924748Z",
     "shell.execute_reply.started": "2025-10-09T14:25:07.920968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def gradcam_heatmap(img_raw, target_idx):\n",
    "    x = convnext_preprocess(img_raw[None, ...])   # preprocess like training\n",
    "    cam_small = _gradcam_core(x, target_idx, feat_model, head_model)[0]\n",
    "    cam = tf.image.resize(cam_small[..., None], IMG_SIZE)[..., 0].numpy()\n",
    "    return np.maximum(cam, 0) / (cam.max() + 1e-8)   # normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T14:25:09.675755Z",
     "iopub.status.busy": "2025-10-09T14:25:09.675496Z",
     "iopub.status.idle": "2025-10-09T14:25:09.679959Z",
     "shell.execute_reply": "2025-10-09T14:25:09.679311Z",
     "shell.execute_reply.started": "2025-10-09T14:25:09.675736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def overlay_heatmap(img_raw, cam, alpha=0.4):\n",
    "    heatmap = plt.cm.jet(cam)[..., :3] * 255.0\n",
    "    overlay = (1 - alpha) * img_raw + alpha * heatmap\n",
    "    return np.clip(overlay, 0, 255).astype(\"uint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T14:33:26.863207Z",
     "iopub.status.busy": "2025-10-09T14:33:26.862914Z",
     "iopub.status.idle": "2025-10-09T15:09:18.210509Z",
     "shell.execute_reply": "2025-10-09T15:09:18.209728Z",
     "shell.execute_reply.started": "2025-10-09T14:33:26.863184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function conv.<locals>._conv_xla at 0x37184dca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function conv.<locals>._conv_xla at 0x37184d820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_paths):\n\u001b[1;32m      6\u001b[0m     img_raw \u001b[38;5;241m=\u001b[39m load_img_for_convnext(path)\n\u001b[0;32m----> 7\u001b[0m     pred, conf, ent, p \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_raw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     y_true\u001b[38;5;241m.\u001b[39mappend(val_labels[i])\n\u001b[1;32m      9\u001b[0m     y_pred\u001b[38;5;241m.\u001b[39mappend(pred)\n",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m, in \u001b[0;36mpredict_softmax\u001b[0;34m(img_raw)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_softmax\u001b[39m(img_raw):\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m convnext_preprocess(img_raw[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])    \u001b[38;5;66;03m# preprocess inside\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]        \u001b[38;5;66;03m# (C,)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(p))\n\u001b[1;32m     16\u001b[0m     conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmax(p))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/layers/layer.py:934\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_scope \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m new_scope:\n\u001b[0;32m--> 934\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/ops/operation.py:58\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     54\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     55\u001b[0m         call_fn,\n\u001b[1;32m     56\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     57\u001b[0m     )\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/models/functional.py:183\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m             backend\u001b[38;5;241m.\u001b[39mset_keras_mask(x, mask)\n\u001b[0;32m--> 183\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_through_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unpack_singleton(outputs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/ops/function.py:177\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[0;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[1;32m    175\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39moutputs, tree\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/models/functional.py:648\u001b[0m, in \u001b[0;36moperation_fn.<locals>.call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    643\u001b[0m         name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(operation, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_call_context_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     ):\n\u001b[1;32m    646\u001b[0m         kwargs[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moperation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/layers/layer.py:936\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/ops/operation.py:58\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     54\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     55\u001b[0m         call_fn,\n\u001b[1;32m     56\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     57\u001b[0m     )\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/models/functional.py:183\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m             backend\u001b[38;5;241m.\u001b[39mset_keras_mask(x, mask)\n\u001b[0;32m--> 183\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_through_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unpack_singleton(outputs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/ops/function.py:177\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[0;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[1;32m    175\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39moutputs, tree\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/models/functional.py:648\u001b[0m, in \u001b[0;36moperation_fn.<locals>.call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    643\u001b[0m         name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(operation, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_call_context_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     ):\n\u001b[1;32m    646\u001b[0m         kwargs[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moperation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/layers/layer.py:936\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/ops/operation.py:58\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     54\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     55\u001b[0m         call_fn,\n\u001b[1;32m     56\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     57\u001b[0m     )\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/layers/convolutional/base_conv.py:250\u001b[0m, in \u001b[0;36mBaseConv.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m--> 250\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels_last\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/layers/convolutional/base_conv.py:240\u001b[0m, in \u001b[0;36mBaseConv.convolution_op\u001b[0;34m(self, inputs, kernel)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvolution_op\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, kernel):\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdilation_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/ops/nn.py:1342\u001b[0m, in \u001b[0;36mconv\u001b[0;34m(inputs, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((inputs,)):\n\u001b[1;32m   1339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Conv(strides, padding, data_format, dilation_rate)\u001b[38;5;241m.\u001b[39msymbolic_call(\n\u001b[1;32m   1340\u001b[0m         inputs, kernel\n\u001b[1;32m   1341\u001b[0m     )\n\u001b[0;32m-> 1342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation_rate\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/nn.py:338\u001b[0m, in \u001b[0;36mconv\u001b[0;34m(inputs, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m    336\u001b[0m needs_xla \u001b[38;5;241m=\u001b[39m needs_xla \u001b[38;5;129;01mor\u001b[39;00m channels \u001b[38;5;241m!=\u001b[39m kernel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_xla:\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_conv_xla\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _conv()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m   filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    915\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(\n\u001b[1;32m    916\u001b[0m           bound_args\n\u001b[1;32m    917\u001b[0m       )\n\u001b[1;32m    918\u001b[0m   )\n\u001b[0;32m--> 919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    920\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[1;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1703\u001b[0m   )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Predict entire validation fold\n",
    "y_true, y_pred = [], []\n",
    "y_prob = np.zeros((len(val_paths), N_CLASSES), dtype=np.float32)\n",
    "\n",
    "for i, path in enumerate(val_paths):\n",
    "    img_raw = load_img_for_convnext(path)\n",
    "    pred, conf, ent, p = predict_softmax(img_raw)\n",
    "    y_true.append(val_labels[i])\n",
    "    y_pred.append(pred)\n",
    "    y_prob[i] = p\n",
    "\n",
    "y_true = np.array(y_true, int)\n",
    "y_pred = np.array(y_pred, int)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(N_CLASSES)))\n",
    "cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm_norm, vmin=0, vmax=1,cmap=\"viridis\")\n",
    "plt.title(\"Normalized Confusion Matrix (Val)\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.xticks(range(N_CLASSES), class_names, rotation=45, ha='right')\n",
    "plt.yticks(range(N_CLASSES), class_names)\n",
    "for i in range(N_CLASSES):\n",
    "    for j in range(N_CLASSES):\n",
    "        plt.text(j, i, f\"{cm_norm[i,j]:.2f}\", ha='center', va='center', fontsize=8, color='white' if cm_norm[i,j]>0.5 else 'black')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix.png\"), dpi=140)\n",
    "plt.show()\n",
    "\n",
    "# Classification report (per-class precision/recall/F1)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T15:15:41.086022Z",
     "iopub.status.busy": "2025-10-09T15:15:41.085269Z",
     "iopub.status.idle": "2025-10-09T15:15:41.481494Z",
     "shell.execute_reply": "2025-10-09T15:15:41.480738Z",
     "shell.execute_reply.started": "2025-10-09T15:15:41.085996Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [219, 1280]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m fpr, tpr, roc_auc \u001b[38;5;241m=\u001b[39m {}, {}, {}\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_CLASSES):\n\u001b[0;32m----> 6\u001b[0m     fpr[c], tpr[c], _ \u001b[38;5;241m=\u001b[39m \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_true_bin\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_prob\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     roc_auc[c] \u001b[38;5;241m=\u001b[39m auc(fpr[c], tpr[c])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Micro/macro\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:1150\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1047\u001b[0m     {\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m ):\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;124;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1150\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:820\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m--> 820\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n\u001b[1;32m    822\u001b[0m y_score \u001b[38;5;241m=\u001b[39m column_or_1d(y_score)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    478\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [219, 1280]"
     ]
    }
   ],
   "source": [
    "# Binarize labels for ROC\n",
    "Y_true_bin = label_binarize(y_true, classes=list(range(N_CLASSES)))  # shape (N, C)\n",
    "\n",
    "fpr, tpr, roc_auc = {}, {}, {}\n",
    "for c in range(N_CLASSES):\n",
    "    fpr[c], tpr[c], _ = roc_curve(Y_true_bin[:, c], y_prob[:, c])\n",
    "    roc_auc[c] = auc(fpr[c], tpr[c])\n",
    "\n",
    "# Micro/macro\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_true_bin.ravel(), y_prob.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "all_fpr = np.unique(np.concatenate([fpr[c] for c in range(N_CLASSES)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for c in range(N_CLASSES):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[c], tpr[c])\n",
    "mean_tpr /= N_CLASSES\n",
    "roc_auc[\"macro\"] = auc(all_fpr, mean_tpr)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "for c in range(N_CLASSES):\n",
    "    plt.plot(fpr[c], tpr[c], lw=1, label=f\"{class_names[c]} (AUC={roc_auc[c]:.3f})\")\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], lw=2, label=f\"micro (AUC={roc_auc['micro']:.3f})\", linestyle='--')\n",
    "plt.plot([0,1],[0,1], 'k--', lw=1)\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (Val)\")\n",
    "plt.legend(fontsize=7, loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curves.png\"), dpi=140)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T15:15:46.361969Z",
     "iopub.status.busy": "2025-10-09T15:15:46.361288Z",
     "iopub.status.idle": "2025-10-09T15:15:46.696653Z",
     "shell.execute_reply": "2025-10-09T15:15:46.695931Z",
     "shell.execute_reply.started": "2025-10-09T15:15:46.361945Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Reliability / calibration: compare confidence vs accuracy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m conf \u001b[38;5;241m=\u001b[39m y_prob\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)                 \u001b[38;5;66;03m# predicted confidence\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m(\u001b[38;5;28mint\u001b[39m)  \u001b[38;5;66;03m# 1/0 per sample\u001b[39;00m\n\u001b[1;32m      5\u001b[0m bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m11\u001b[39m)  \u001b[38;5;66;03m# 10 bins\u001b[39;00m\n\u001b[1;32m      6\u001b[0m bin_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdigitize(conf, bins) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "# Reliability / calibration: compare confidence vs accuracy\n",
    "conf = y_prob.max(axis=1)                 # predicted confidence\n",
    "correct = (y_pred == y_true).astype(int)  # 1/0 per sample\n",
    "\n",
    "bins = np.linspace(0.0, 1.0, 11)  # 10 bins\n",
    "bin_ids = np.digitize(conf, bins) - 1\n",
    "bin_acc, bin_conf, bin_count = [], [], []\n",
    "\n",
    "ECE = 0.0\n",
    "N = len(conf)\n",
    "for b in range(len(bins)-1):\n",
    "    idx = (bin_ids == b)\n",
    "    if np.any(idx):\n",
    "        acc_b = correct[idx].mean()\n",
    "        conf_b = conf[idx].mean()\n",
    "        cnt_b = idx.sum()\n",
    "        ECE += (cnt_b / N) * abs(acc_b - conf_b)\n",
    "        bin_acc.append(acc_b); bin_conf.append(conf_b); bin_count.append(cnt_b)\n",
    "    else:\n",
    "        bin_acc.append(np.nan); bin_conf.append(np.nan); bin_count.append(0)\n",
    "\n",
    "# Plot\n",
    "centers = 0.5*(bins[:-1] + bins[1:])\n",
    "plt.figure(figsize=(5.2,5))\n",
    "plt.plot([0,1],[0,1],'k--',label='Perfect calibration')\n",
    "plt.scatter(centers, bin_acc, s=np.array(bin_count)*2+5, label='Observed acc')\n",
    "plt.plot(centers, bin_conf, label='Mean conf', alpha=0.7)\n",
    "plt.xlim(0,1); plt.ylim(0,1)\n",
    "plt.xlabel(\"Confidence\"); plt.ylabel(\"Accuracy\")\n",
    "plt.title(f\"Reliability Diagram (ECE={ECE:.3f})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"reliability_diagram.png\"), dpi=140)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Expected Calibration Error (ECE): {ECE:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T13:29:13.400613Z",
     "iopub.status.busy": "2025-10-10T13:29:13.400324Z",
     "iopub.status.idle": "2025-10-10T15:04:42.433909Z",
     "shell.execute_reply": "2025-10-10T15:04:42.433221Z",
     "shell.execute_reply.started": "2025-10-10T13:29:13.400578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) Imports\n",
    "# =========================\n",
    "import os, math, numpy as np, tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses, metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "# Optional: mixed precision for speed on P100 (usually beneficial)\n",
    "set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# Keras ConvNeXt (requires TF/Keras >= 2.11)\n",
    "from tensorflow.keras.applications import ConvNeXtTiny\n",
    "from tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\n",
    "\n",
    "# =========================\n",
    "# 1) Config\n",
    "# =========================\n",
    "IMG_SIZE = (224, 224)         # ConvNeXt default\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_WARMUP = 3             # freeze backbone first\n",
    "EPOCHS_FINETUNE = 12          # then unfreeze\n",
    "LR_WARMUP = 1e-3\n",
    "LR_FINETUNE = 5e-4\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "N_FOLDS = len(folds_data)     # should be 5 for your setup\n",
    "N_CLASSES = len(class_names)\n",
    "\n",
    "# =========================\n",
    "# 2) Data pipeline (with MixUp/CutMix; ConvNeXt preprocess)\n",
    "# =========================\n",
    "def decode_img_for_convnext(file_path, label):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)  # change to decode_png if needed\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(img, tf.float32)               # keep [0..255] for convnext_preprocess\n",
    "    return img, label\n",
    "\n",
    "# light augmentations\n",
    "data_augment = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomContrast(0.1),\n",
    "], name=\"augment\")\n",
    "\n",
    "def apply_preprocess(x, y):\n",
    "    x = convnext_preprocess(tf.cast(x, tf.float32))  # ConvNeXt normalization\n",
    "    return x, y\n",
    "\n",
    "# --- MixUp & CutMix (custom implementation that outputs one-hot labels) ---\n",
    "def _sample_beta(alpha=0.2):\n",
    "    # Beta(alpha, alpha) via Gamma sampling (scalar)\n",
    "    g1 = tf.random.gamma(shape=[1], alpha=alpha)\n",
    "    g2 = tf.random.gamma(shape=[1], alpha=alpha)\n",
    "    lam = g1 / (g1 + g2)\n",
    "    return tf.cast(lam[0], tf.float32)\n",
    "\n",
    "def _mixup_pair(x1, y1, x2, y2, alpha=0.2):\n",
    "    lam = _sample_beta(alpha)\n",
    "    x = lam * x1 + (1.0 - lam) * x2\n",
    "    y1 = tf.one_hot(y1, N_CLASSES)\n",
    "    y2 = tf.one_hot(y2, N_CLASSES)\n",
    "    y = lam * y1 + (1.0 - lam) * y2\n",
    "    return x, y\n",
    "\n",
    "def _cutmix_pair(x1, y1, x2, y2, alpha=1.0):\n",
    "    lam = _sample_beta(alpha)\n",
    "    h, w = IMG_SIZE\n",
    "    cut_w = tf.cast(w * tf.sqrt(1.0 - lam), tf.int32)\n",
    "    cut_h = tf.cast(h * tf.sqrt(1.0 - lam), tf.int32)\n",
    "    # random center\n",
    "    cx = tf.random.uniform((), 0, w, dtype=tf.int32)\n",
    "    cy = tf.random.uniform((), 0, h, dtype=tf.int32)\n",
    "    x1_1 = tf.clip_by_value(cx - cut_w // 2, 0, w)\n",
    "    y1_1 = tf.clip_by_value(cy - cut_h // 2, 0, h)\n",
    "    x2_1 = tf.clip_by_value(cx + cut_w // 2, 0, w)\n",
    "    y2_1 = tf.clip_by_value(cy + cut_h // 2, 0, h)\n",
    "    # mask\n",
    "    pad_top, pad_bottom = y1_1, h - y2_1\n",
    "    pad_left, pad_right = x1_1, w - x2_1\n",
    "    mask = tf.pad(tf.ones((y2_1 - y1_1, x2_1 - x1_1, 3), dtype=tf.float32),\n",
    "                  [[pad_top, pad_bottom], [pad_left, pad_right], [0, 0]])\n",
    "    x = x1 * (1.0 - mask) + x2 * mask\n",
    "    lam_adj = 1.0 - tf.cast((x2_1 - x1_1) * (y2_1 - y1_1), tf.float32) / tf.cast(h * w, tf.float32)\n",
    "    y1 = tf.one_hot(y1, N_CLASSES)\n",
    "    y2 = tf.one_hot(y2, N_CLASSES)\n",
    "    y = lam_adj * y1 + (1.0 - lam_adj) * y2\n",
    "    return x, y\n",
    "\n",
    "def _apply_mixup_cutmix_batch(x, y, p_mixup=0.5, p_cutmix=0.5):\n",
    "    # x: (B,H,W,3), y: (B,) int labels\n",
    "    B = tf.shape(x)[0]\n",
    "    # shuffle indices (pair with another sample)\n",
    "    idx = tf.random.shuffle(tf.range(B))\n",
    "    x2, y2 = tf.gather(x, idx), tf.gather(y, idx)\n",
    "\n",
    "    # decide which images use MixUp or CutMix\n",
    "    r = tf.random.uniform((B,), 0, 1)\n",
    "    use_mix = r < p_mixup\n",
    "    use_cut = (r >= p_mixup) & (r < (p_mixup + p_cutmix))\n",
    "    # default one-hot (no mix) for the rest\n",
    "    y_oh = tf.one_hot(y, N_CLASSES)\n",
    "\n",
    "    # vectorized map over batch (tf.map_fn to keep graph)\n",
    "    def _per_sample(args):\n",
    "        xi, yi, xj, yj, use_m, use_c = args\n",
    "        def do_mix():\n",
    "            return _mixup_pair(xi, yi, xj, yj, alpha=0.2)\n",
    "        def do_cut():\n",
    "            return _cutmix_pair(xi, yi, xj, yj, alpha=1.0)\n",
    "        def no_mix():\n",
    "            return xi, tf.one_hot(yi, N_CLASSES)\n",
    "        return tf.case(\n",
    "            [(use_m, do_mix), (use_c, do_cut)],\n",
    "            default=no_mix, exclusive=True\n",
    "        )\n",
    "    x_mixed, y_mixed = tf.map_fn(\n",
    "        _per_sample, (x, y, x2, y2, use_mix, use_cut),\n",
    "        fn_output_signature=(tf.float32, tf.float32)\n",
    "    )\n",
    "    return x_mixed, y_mixed\n",
    "\n",
    "def make_tf_dataset_convnext(file_paths, labels, training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    if training:\n",
    "        ds = ds.shuffle(len(file_paths), reshuffle_each_iteration=True)\n",
    "\n",
    "    # decode + augment\n",
    "    ds = ds.map(decode_img_for_convnext, num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.map(lambda x, y: (data_augment(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    # preprocess first (keeps numeric stability)\n",
    "    ds = ds.map(apply_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    # batch before mix to mix within-batch (important!)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    if training:\n",
    "        # MixUp/CutMix on the batch, then continue\n",
    "        ds = ds.map(lambda x, y: _apply_mixup_cutmix_batch(x, y, 0.5, 0.5), num_parallel_calls=AUTOTUNE)\n",
    "    else:\n",
    "        # one-hot labels for validation to match CategoricalCrossentropy\n",
    "        ds = ds.map(lambda x, y: (x, tf.one_hot(y, N_CLASSES)), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    ds = ds.prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# =========================\n",
    "# 3) ConvNeXt-Tiny model builder\n",
    "# =========================\n",
    "def build_convnext_tiny(num_classes=N_CLASSES, image_size=IMG_SIZE, trainable_backbone=False):\n",
    "    # Backbone\n",
    "    backbone = ConvNeXtTiny(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(image_size[0], image_size[1], 3),\n",
    "        pooling=None\n",
    "    )\n",
    "    backbone.trainable = trainable_backbone  # warmup: False; finetune: True\n",
    "\n",
    "    inputs = layers.Input(shape=(image_size[0], image_size[1], 3))\n",
    "    x = backbone(inputs, training=False)\n",
    "\n",
    "    # Head (GAP + BN + Dropout + Dense)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)  # keep float32 for stability\n",
    "\n",
    "    model = models.Model(inputs, outputs, name=\"ConvNeXtTiny_Kvasir\")\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# 4) Training utilities (fixed LR for warmup, Cosine for finetune)\n",
    "# =========================\n",
    "def compile_model_warmup(model, lr):\n",
    "    opt = optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=losses.CategoricalCrossentropy(),  # using one-hot labels everywhere\n",
    "        metrics=[metrics.CategoricalAccuracy(name=\"acc\")]\n",
    "    )\n",
    "\n",
    "def compile_model_cosine(model, initial_lr, epochs, steps_per_epoch):\n",
    "    cosine = optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=initial_lr,\n",
    "        decay_steps=epochs * steps_per_epoch,\n",
    "        alpha=1e-6 / initial_lr  # final LR = initial_lr * alpha\n",
    "    )\n",
    "    opt = optimizers.Adam(learning_rate=cosine)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=losses.CategoricalCrossentropy(),\n",
    "        metrics=[metrics.CategoricalAccuracy(name=\"acc\")]\n",
    "    )\n",
    "\n",
    "def get_callbacks(fold_id):\n",
    "    os.makedirs(\"convnext_runs\", exist_ok=True)\n",
    "    return [\n",
    "        EarlyStopping(monitor=\"val_acc\", patience=6, mode=\"max\", restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.5, patience=3, verbose=1, min_lr=1e-6),\n",
    "        ModelCheckpoint(\n",
    "            filepath=f\"convnext_runs/best_convnextT_fold{fold_id}.h5\",\n",
    "            monitor=\"val_acc\", mode=\"max\", save_best_only=True, verbose=1\n",
    "        ),\n",
    "        CSVLogger(f\"convnext_runs/training_log_fold{fold_id}.csv\", append=False)\n",
    "    ]\n",
    "\n",
    "# =========================\n",
    "# 5) 5-Fold training (warmup + finetune)\n",
    "# =========================\n",
    "fold_metrics = []\n",
    "\n",
    "def get_callbacks(fold_id):\n",
    "    os.makedirs(\"convnext_runs\", exist_ok=True)\n",
    "    return [\n",
    "        EarlyStopping(monitor=\"val_acc\", patience=6, mode=\"max\", restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint(\n",
    "            filepath=f\"convnext_runs/best_convnextT_fold{fold_id}.h5\",\n",
    "            monitor=\"val_acc\", mode=\"max\", save_best_only=True, verbose=1\n",
    "        ),\n",
    "        CSVLogger(f\"convnext_runs/training_log_fold{fold_id}.csv\", append=False)\n",
    "    ]\n",
    "\n",
    "for fold_id in range(N_FOLDS):\n",
    "    print(f\"\\n================= FOLD {fold_id+1}/{N_FOLDS} =================\")\n",
    "\n",
    "    train_paths = folds_data[fold_id]['train_paths']\n",
    "    train_labels = folds_data[fold_id]['train_labels']\n",
    "    val_paths   = folds_data[fold_id]['val_paths']\n",
    "    val_labels  = folds_data[fold_id]['val_labels']\n",
    "\n",
    "    train_ds = make_tf_dataset_convnext(train_paths, train_labels, training=True)\n",
    "    val_ds   = make_tf_dataset_convnext(val_paths,   val_labels,   training=False)\n",
    "\n",
    "    # 5.1 Warmup: freeze backbone\n",
    "    model = build_convnext_tiny(trainable_backbone=False)\n",
    "    compile_model_warmup(model, lr=LR_WARMUP)\n",
    "    cb = get_callbacks(fold_id)\n",
    "\n",
    "    print(\"\\n---- Warmup (frozen backbone) ----\")\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS_WARMUP,\n",
    "        callbacks=cb,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 5.2 Fine-tune: unfreeze backbone + cosine annealing\n",
    "    model.get_layer(index=1).trainable = True\n",
    "    compile_model_cosine(model, initial_lr=LR_FINETUNE, epochs=EPOCHS_FINETUNE, steps_per_epoch=len(train_ds))\n",
    "    cb = get_callbacks(fold_id)\n",
    "\n",
    "    print(\"\\n---- Fine-tuning (unfrozen backbone, cosine annealing) ----\")\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS_FINETUNE,\n",
    "        callbacks=cb,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate best weights\n",
    "    eval_res = model.evaluate(val_ds, verbose=0)\n",
    "    fold_metrics.append({\"fold\": fold_id+1, \"val_loss\": float(eval_res[0]), \"val_acc\": float(eval_res[1])})\n",
    "    print(f\"FOLD {fold_id+1} → val_loss: {eval_res[0]:.4f} | val_acc: {eval_res[1]:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# 6) Summary across folds\n",
    "# =========================\n",
    "val_accs  = [m[\"val_acc\"]  for m in fold_metrics]\n",
    "val_losses= [m[\"val_loss\"] for m in fold_metrics]\n",
    "\n",
    "print(\"\\n========= 5-Fold Summary (ConvNeXt-T + MixUp/CutMix + CosineLR) =========\")\n",
    "for m in fold_metrics:\n",
    "    print(f\"Fold {m['fold']}:  val_acc={m['val_acc']:.4f},  val_loss={m['val_loss']:.4f}\")\n",
    "\n",
    "print(f\"\\nMean val_acc: {np.mean(val_accs):.4f}  (± {np.std(val_accs):.4f})\")\n",
    "print(f\"Mean val_loss: {np.mean(val_losses):.4f} (± {np.std(val_losses):.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:31:52.450566Z",
     "iopub.status.busy": "2025-10-10T15:31:52.450275Z",
     "iopub.status.idle": "2025-10-10T15:31:54.554708Z",
     "shell.execute_reply": "2025-10-10T15:31:54.553997Z",
     "shell.execute_reply.started": "2025-10-10T15:31:52.450546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Inline visualization of training/validation curves from CSVLogger outputs\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "LOG_DIR = Path(\"/kaggle/working/convnext_runs\")\n",
    "N_FOLDS = 5  # change if needed\n",
    "\n",
    "def pick_cols(df):\n",
    "    m = {c.lower().strip(): c for c in df.columns}\n",
    "    acc = m.get(\"acc\") or m.get(\"accuracy\") or m.get(\"sparse_categorical_accuracy\") or m.get(\"categorical_accuracy\")\n",
    "    val_acc = m.get(\"val_acc\") or m.get(\"val_accuracy\") or m.get(\"val_sparse_categorical_accuracy\") or m.get(\"val_categorical_accuracy\")\n",
    "    loss = m.get(\"loss\")\n",
    "    val_loss = m.get(\"val_loss\")\n",
    "    return acc, val_acc, loss, val_loss\n",
    "\n",
    "# --- Per-fold panels ---\n",
    "for f in range(N_FOLDS):\n",
    "    csvp = LOG_DIR / f\"training_log_fold{f}.csv\"\n",
    "    if not csvp.exists():\n",
    "        print(f\"[skip] {csvp} not found\"); \n",
    "        continue\n",
    "    df = pd.read_csv(csvp)\n",
    "    acc, val_acc, loss, val_loss = pick_cols(df)\n",
    "    epochs = np.arange(1, len(df)+1)\n",
    "\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(7,4))\n",
    "    if acc:     plt.plot(epochs, df[acc], label=\"train acc\")\n",
    "    if val_acc: plt.plot(epochs, df[val_acc], label=\"val acc\")\n",
    "    plt.title(f\"Fold {f} • Accuracy\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.grid(True, linewidth=0.3); plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Loss\n",
    "    plt.figure(figsize=(7,4))\n",
    "    if loss:     plt.plot(epochs, df[loss], label=\"train loss\")\n",
    "    if val_loss: plt.plot(epochs, df[val_loss], label=\"val loss\")\n",
    "    plt.title(f\"Fold {f} • Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.grid(True, linewidth=0.3); plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# --- All-folds comparison (validation curves only) ---\n",
    "plt.figure(figsize=(8,5))\n",
    "for f in range(N_FOLDS):\n",
    "    csvp = LOG_DIR / f\"training_log_fold{f}.csv\"\n",
    "    if not csvp.exists(): continue\n",
    "    df = pd.read_csv(csvp)\n",
    "    _, val_acc, _, val_loss = pick_cols(df)\n",
    "    if val_acc: plt.plot(np.arange(1, len(df)+1), df[val_acc], label=f\"Fold {f}\")\n",
    "plt.title(\"Validation Accuracy — All Folds\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Accuracy\"); plt.grid(True, linewidth=0.3); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for f in range(N_FOLDS):\n",
    "    csvp = LOG_DIR / f\"training_log_fold{f}.csv\"\n",
    "    if not csvp.exists(): continue\n",
    "    df = pd.read_csv(csvp)\n",
    "    _, _, _, val_loss = pick_cols(df)\n",
    "    if val_loss: plt.plot(np.arange(1, len(df)+1), df[val_loss], label=f\"Fold {f}\")\n",
    "plt.title(\"Validation Loss — All Folds\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Loss\"); plt.grid(True, linewidth=0.3); plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:33:12.731180Z",
     "iopub.status.busy": "2025-10-10T15:33:12.730926Z",
     "iopub.status.idle": "2025-10-10T15:33:28.920880Z",
     "shell.execute_reply": "2025-10-10T15:33:28.920114Z",
     "shell.execute_reply.started": "2025-10-10T15:33:12.731163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============== Robust Grad-CAM (manual forward; heatmap only) ===============\n",
    "import os, glob, random, numpy as np, tensorflow as tf, cv2\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ConvNeXtTiny\n",
    "from tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATA_ROOT = \"/kaggle/input/kvasir-final-preprocessed-data/kvasir_bilateral_filtered\"\n",
    "WEIGHTS   = Path(\"/kaggle/working/convnext_runs/best_convnextT_fold4.h5\")  # change fold if you want\n",
    "IMG_SIZE  = (224, 224)\n",
    "N_CLASSES = 8\n",
    "TARGET    = \"pred\"  # \"pred\" or an int class id\n",
    "OUT_DIR   = Path(\"/kaggle/working/gradcam_heatmaps\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Build SAME head you trained and load weights ---\n",
    "def build_model(nc=N_CLASSES, image_size=IMG_SIZE):\n",
    "    bb = ConvNeXtTiny(include_top=False, weights=None,\n",
    "                      input_shape=(image_size[0], image_size[1], 3), pooling=None)\n",
    "    inp = layers.Input(shape=(image_size[0], image_size[1], 3))\n",
    "    x = bb(inp, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(nc, activation=\"softmax\", dtype=\"float32\")(x)\n",
    "    return models.Model(inp, out)\n",
    "\n",
    "model = build_model()\n",
    "model.load_weights(str(WEIGHTS))\n",
    "\n",
    "# --- Pick a real image (or set IMG_PATH yourself) ---\n",
    "def pick_image(root):\n",
    "    exts = (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\",\"*.JPEG\")\n",
    "    c = []\n",
    "    for e in exts: c += glob.glob(os.path.join(root, \"**\", e), recursive=True)\n",
    "    assert c, f\"No images under {root}\"\n",
    "    return random.choice(c)\n",
    "\n",
    "IMG_PATH = pick_image(DATA_ROOT)\n",
    "print(\"Using image:\", IMG_PATH)\n",
    "\n",
    "# --- Preprocess ---\n",
    "def load_for_model(fp, size):\n",
    "    img = tf.io.read_file(fp)\n",
    "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, size)\n",
    "    x = tf.cast(img, tf.float32)          # [0..255]\n",
    "    x = convnext_preprocess(x)            # ConvNeXt preprocessing\n",
    "    return tf.expand_dims(x, 0), tf.cast(img, tf.uint8)\n",
    "\n",
    "x, img_uint8 = load_for_model(IMG_PATH, IMG_SIZE)\n",
    "\n",
    "# --- Helpers ---\n",
    "def find_last_4d_layer_name(m):\n",
    "    # walk from end to start to get the deepest 4D (HWC) tensor layer\n",
    "    for lyr in reversed(m.layers):\n",
    "        shp = getattr(lyr, \"output_shape\", None)\n",
    "        if isinstance(shp, tuple) and len(shp) == 4:\n",
    "            return lyr.name\n",
    "    raise ValueError(\"No 4D conv feature layer found for Grad-CAM.\")\n",
    "\n",
    "LAST_NAME = find_last_4d_layer_name(model)\n",
    "\n",
    "def gradcam_manual(m, x, class_idx, last_name, img_size):\n",
    "    \"\"\"\n",
    "    Manual forward through layers inside one GradientTape to avoid Keras-3 multi-output issues.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        t = x\n",
    "        conv_out = None\n",
    "        for lyr in m.layers[1:]:  # skip InputLayer at index 0\n",
    "            t = lyr(t, training=False)\n",
    "            if lyr.name == last_name:\n",
    "                conv_out = t\n",
    "        preds = t                            # (1, C)\n",
    "        target = preds[:, class_idx]         # scalar per sample\n",
    "    assert conv_out is not None, \"Last conv feature not found during forward pass.\"\n",
    "    grads   = tape.gradient(target, conv_out)          # (1, h, w, C)\n",
    "    weights = tf.reduce_mean(grads, axis=(0,1,2))      # (C,)\n",
    "    cam     = tf.tensordot(conv_out[0], weights, axes=1)  # (h, w)\n",
    "    cam     = tf.nn.relu(cam)\n",
    "    cam     = cam / (tf.reduce_max(cam) + 1e-12)\n",
    "    cam     = tf.image.resize(cam[...,None], img_size)[...,0]\n",
    "    return cam.numpy(), preds.numpy()[0]\n",
    "\n",
    "# --- First pass to get predicted class (then CAM for it) ---\n",
    "cam_tmp, probs0 = gradcam_manual(model, x, class_idx=0, last_name=LAST_NAME, img_size=IMG_SIZE)\n",
    "pred_id = int(np.argmax(probs0))\n",
    "class_idx = pred_id if TARGET == \"pred\" else int(TARGET)\n",
    "\n",
    "# --- Final CAM for chosen class ---\n",
    "cam, probs = gradcam_manual(model, x, class_idx=class_idx, last_name=LAST_NAME, img_size=IMG_SIZE)\n",
    "\n",
    "# --- Save grayscale heatmap ---\n",
    "out_path = OUT_DIR / f\"cam_{Path(IMG_PATH).stem}_class{class_idx}.png\"\n",
    "cv2.imwrite(str(out_path), (np.clip(cam,0,1)*255).astype(np.uint8))\n",
    "print(f\"Saved heatmap → {out_path}\")\n",
    "print(\"Predicted class id:\", pred_id, \"| CAM for class id:\", class_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:33:38.876679Z",
     "iopub.status.busy": "2025-10-10T15:33:38.876380Z",
     "iopub.status.idle": "2025-10-10T15:33:48.084249Z",
     "shell.execute_reply": "2025-10-10T15:33:48.083617Z",
     "shell.execute_reply.started": "2025-10-10T15:33:38.876654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =====================  One-shot PostSegXAI-style dashboard  =====================\n",
    "# Produces a 2x3 figure: Original | Grad-CAM overlay | Uncertainty(1-CAM)\n",
    "#                        Class probabilities | Confidence regions | Text summary\n",
    "import os, glob, json, random, numpy as np, tensorflow as tf, cv2, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ConvNeXtTiny\n",
    "from tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DATA_ROOT  = \"/kaggle/input/kvasir-final-preprocessed-data/kvasir_bilateral_filtered\"\n",
    "WEIGHTS    = \"/kaggle/working/convnext_runs/best_convnextT_fold4.h5\"  # change fold if you want\n",
    "IMG_PATH   = None   # put a specific file path, or leave None and we'll auto-pick\n",
    "IMG_SIZE   = (224, 224)\n",
    "N_CLASSES  = 8      # set to your dataset\n",
    "ALPHA      = 0.45   # overlay strength\n",
    "\n",
    "# Optional: class names (edit if you want pretty labels)\n",
    "# If you saved meta.json earlier, we will try to read from it automatically.\n",
    "CLASS_NAMES = [\"dyed-lifted-polyps\",\"dyed-resection-margins\",\"esophagitis\",\"normal-cecum\",\n",
    "               \"normal-pylorus\",\"normal-z-line\",\"polyps\",\"ulcerative\"]  # <-- change to your order if needed\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "def pick_image(root):\n",
    "    exts = (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\",\"*.JPEG\")\n",
    "    c = []\n",
    "    for e in exts: c += glob.glob(os.path.join(root, \"**\", e), recursive=True)\n",
    "    assert c, f\"No images under {root}\"\n",
    "    return random.choice(c)\n",
    "\n",
    "if IMG_PATH is None:\n",
    "    IMG_PATH = pick_image(DATA_ROOT)\n",
    "\n",
    "# Try reading class names from a nearby meta.json if present\n",
    "META_CAND = list(Path(\"/kaggle/working\").glob(\"convnext_artifacts_*/meta.json\"))\n",
    "if META_CAND:\n",
    "    try:\n",
    "        meta = json.load(open(META_CAND[-1]))\n",
    "        if \"class_names\" in meta and len(meta[\"class_names\"]) == N_CLASSES:\n",
    "            CLASS_NAMES = meta[\"class_names\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# True label (optional) from folder name in dataset\n",
    "true_label_name = Path(IMG_PATH).parent.name  # e.g., \"normal-z-line\" if folder name equals class name\n",
    "true_label_idx  = CLASS_NAMES.index(true_label_name) if true_label_name in CLASS_NAMES else None\n",
    "\n",
    "# Build SAME head you trained & load weights\n",
    "def build_model(nc=N_CLASSES, image_size=IMG_SIZE):\n",
    "    bb = ConvNeXtTiny(include_top=False, weights=None,\n",
    "                      input_shape=(image_size[0], image_size[1], 3), pooling=None)\n",
    "    inp = layers.Input(shape=(image_size[0], image_size[1], 3))\n",
    "    x = bb(inp, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(nc, activation=\"softmax\", dtype=\"float32\")(x)\n",
    "    return models.Model(inp, out)\n",
    "\n",
    "model = build_model()\n",
    "model.load_weights(WEIGHTS)\n",
    "\n",
    "def load_for_model(fp, size):\n",
    "    img_bgr = cv2.imread(fp)  # BGR\n",
    "    if img_bgr is None:\n",
    "        # fallback via tf if OpenCV can't read\n",
    "        img = tf.io.read_file(fp)\n",
    "        img = tf.io.decode_image(img, channels=3, expand_animations=False).numpy()[:, :, ::-1]  # RGB\n",
    "    else:\n",
    "        img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, size, interpolation=cv2.INTER_AREA)\n",
    "    x = img.astype(np.float32)\n",
    "    x = convnext_preprocess(x)            # ConvNeXt preprocessing\n",
    "    return img.astype(np.uint8), np.expand_dims(x, 0)   # (H,W,3) uint8, (1,H,W,3) float\n",
    "\n",
    "orig_rgb, x = load_for_model(IMG_PATH, IMG_SIZE)\n",
    "\n",
    "# ---- Robust Grad-CAM (manual forward; avoids Keras 3 multi-output quirks)\n",
    "def find_last_4d_layer_name(m):\n",
    "    for lyr in reversed(m.layers):\n",
    "        shp = getattr(lyr, \"output_shape\", None)\n",
    "        if isinstance(shp, tuple) and len(shp) == 4:\n",
    "            return lyr.name\n",
    "    raise ValueError(\"No 4D conv feature layer found.\")\n",
    "\n",
    "LAST_NAME = find_last_4d_layer_name(model)\n",
    "\n",
    "def gradcam_manual(m, x, class_idx, last_name, img_size):\n",
    "    with tf.GradientTape() as tape:\n",
    "        t = x\n",
    "        conv_out = None\n",
    "        for lyr in m.layers[1:]:  # skip InputLayer at 0\n",
    "            t = lyr(t, training=False)\n",
    "            if lyr.name == last_name:\n",
    "                conv_out = t\n",
    "        preds = t  # (1, C)\n",
    "        target = preds[:, class_idx]\n",
    "    assert conv_out is not None, \"Could not capture last conv output.\"\n",
    "    grads   = tape.gradient(target, conv_out)          # (1,h,w,C)\n",
    "    weights = tf.reduce_mean(grads, axis=(0,1,2))      # (C,)\n",
    "    cam     = tf.tensordot(conv_out[0], weights, axes=1)\n",
    "    cam     = tf.nn.relu(cam)\n",
    "    cam     = cam / (tf.reduce_max(cam) + 1e-12)\n",
    "    cam     = tf.image.resize(cam[...,None], img_size)[...,0]\n",
    "    return cam.numpy(), preds.numpy()[0]\n",
    "\n",
    "# First pass to get prediction\n",
    "_ , probs0 = gradcam_manual(model, x, class_idx=0, last_name=LAST_NAME, img_size=IMG_SIZE)\n",
    "pred_idx = int(np.argmax(probs0))\n",
    "pred_conf = float(np.max(probs0))\n",
    "pred_name = CLASS_NAMES[pred_idx] if 0 <= pred_idx < len(CLASS_NAMES) else f\"class {pred_idx}\"\n",
    "\n",
    "# Final CAM for predicted class\n",
    "cam, probs = gradcam_manual(model, x, class_idx=pred_idx, last_name=LAST_NAME, img_size=IMG_SIZE)\n",
    "\n",
    "# -------- Visual parts --------\n",
    "# Grad-CAM overlay\n",
    "hm_u8  = (np.clip(cam, 0, 1) * 255).astype(np.uint8)\n",
    "hm_rgb = cv2.applyColorMap(hm_u8, cv2.COLORMAP_JET)[:, :, ::-1]   # RGB\n",
    "overlay = cv2.addWeighted(orig_rgb, 1.0, hm_rgb, ALPHA, 0)\n",
    "\n",
    "# “Uncertainty” map = (1 - CAM) for a quick visual proxy\n",
    "unc = (1.0 - np.clip(cam, 0, 1))\n",
    "unc_rgb = cv2.applyColorMap((unc * 255).astype(np.uint8), cv2.COLORMAP_TURBO)[:, :, ::-1]\n",
    "\n",
    "# Class probability bar chart\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "gs  = fig.add_gridspec(2, 3, height_ratios=[1,1.05], wspace=0.30, hspace=0.35)\n",
    "\n",
    "# (1) Original\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax1.imshow(orig_rgb); ax1.axis('off'); ax1.set_title(\"Original Image\")\n",
    "\n",
    "# (2) Grad-CAM overlay\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "ax2.imshow(overlay); ax2.axis('off'); ax2.set_title(f\"Grad-CAM\\nPred: {pred_name}\")\n",
    "\n",
    "# (3) Uncertainty map\n",
    "ax3 = fig.add_subplot(gs[0,2])\n",
    "ax3.imshow(unc_rgb); ax3.axis('off'); ax3.set_title(f\"Uncertainty Map\\nConfidence: {pred_conf*100:.2f}%\")\n",
    "\n",
    "# (4) Class probabilities\n",
    "ax4 = fig.add_subplot(gs[1,0])\n",
    "classes = CLASS_NAMES if len(CLASS_NAMES)==len(probs) else [str(i) for i in range(len(probs))]\n",
    "ax4.bar(range(len(probs)), probs, tick_label=classes)\n",
    "ax4.set_ylim(0,1.05)\n",
    "ax4.set_title(\"Class Probabilities\")\n",
    "ax4.tick_params(axis='x', labelrotation=35)\n",
    "ax4.set_ylabel(\"Probability\")\n",
    "\n",
    "# (5) Confidence regions (column-wise CAM profile)\n",
    "ax5 = fig.add_subplot(gs[1,1])\n",
    "profile = cam.mean(axis=0)  # average over rows -> per-column confidence\n",
    "H, W = cam.shape\n",
    "canvas = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "# draw vertical lines with color by intensity (red->yellow->green)\n",
    "for j, v in enumerate(profile):\n",
    "    if v < 0.05: \n",
    "        continue\n",
    "    color = (int(255*(1-v)), int(255*v), 0)  # R->G\n",
    "    cv2.line(canvas, (j, int(H*(1-v))), (j, H-1), color, 1)\n",
    "ax5.imshow(canvas); ax5.axis('off'); ax5.set_title(\"Confidence Regions\")\n",
    "\n",
    "# (6) Text summary\n",
    "ax6 = fig.add_subplot(gs[1,2]); ax6.axis('off')\n",
    "text_lines = [\n",
    "    \"Prediction Summary:\",\n",
    "    f\"Class: {pred_name}\",\n",
    "    f\"Confidence: {pred_conf*100:.2f}%\",\n",
    "]\n",
    "if true_label_idx is not None:\n",
    "    corr = \"✓\" if pred_idx == true_label_idx else \"✗\"\n",
    "    text_lines += [f\"True Label: {CLASS_NAMES[true_label_idx]}\", f\"Correct: {corr}\"]\n",
    "ax6.text(0.02, 0.98, \"\\n\".join(text_lines), va='top', fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "print(\"Done. (Image:\", IMG_PATH, \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:36:27.382992Z",
     "iopub.status.busy": "2025-10-10T15:36:27.382226Z",
     "iopub.status.idle": "2025-10-10T15:36:28.702417Z",
     "shell.execute_reply": "2025-10-10T15:36:28.701795Z",
     "shell.execute_reply.started": "2025-10-10T15:36:27.382967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "# find any best_convnextT_fold*.h5 file\n",
    "ckpts = [f for f in os.listdir(\"convnext_runs\") if re.match(r\"best_convnextT_fold\\d+\\.h5$\", f)]\n",
    "assert ckpts, \"No .h5 checkpoints found in convnext_runs/\"\n",
    "# pick the highest-numbered fold (or change logic as you like)\n",
    "ckpts_sorted = sorted(ckpts, key=lambda s: int(re.search(r\"fold(\\d+)\", s).group(1)))\n",
    "ckpt = ckpts_sorted[-1]\n",
    "FOLD_ID = int(re.search(r\"fold(\\d+)\", ckpt).group(1))\n",
    "print(\"Loading checkpoint:\", ckpt, \"| FOLD_ID =\", FOLD_ID)\n",
    "\n",
    "model = build_convnext_tiny()\n",
    "model.load_weights(os.path.join(\"convnext_runs\", ckpt))\n",
    "print(\"Model rebuilt + weights loaded.\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:37:04.561311Z",
     "iopub.status.busy": "2025-10-10T15:37:04.560493Z",
     "iopub.status.idle": "2025-10-10T15:37:04.566819Z",
     "shell.execute_reply": "2025-10-10T15:37:04.566039Z",
     "shell.execute_reply.started": "2025-10-10T15:37:04.561285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths for this fold\n",
    "val_paths = folds_data[FOLD_ID]['val_paths']\n",
    "val_labels = folds_data[FOLD_ID]['val_labels']\n",
    "\n",
    "def load_img_for_convnext(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)    # change to decode_png if needed\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(img, tf.float32)                 # [0..255]\n",
    "    return img\n",
    "\n",
    "def predict_softmax(img_raw):\n",
    "    x = convnext_preprocess(img_raw[None, ...])    # preprocess inside\n",
    "    p = model(x, training=False).numpy()[0]        # (C,)\n",
    "    pred = int(np.argmax(p))\n",
    "    conf = float(np.max(p))\n",
    "    # uncertainty via entropy (nats)\n",
    "    entropy = float(-np.sum(p * np.log(p + 1e-12)))\n",
    "    return pred, conf, entropy, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:37:07.844270Z",
     "iopub.status.busy": "2025-10-10T15:37:07.843694Z",
     "iopub.status.idle": "2025-10-10T15:37:07.867986Z",
     "shell.execute_reply": "2025-10-10T15:37:07.867236Z",
     "shell.execute_reply.started": "2025-10-10T15:37:07.844246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Find last 4D feature layer automatically (should be 'convnext_tiny')\n",
    "def get_last_4d_layer(m):\n",
    "    for lyr in reversed(m.layers):\n",
    "        try:\n",
    "            shp = lyr.output_shape\n",
    "        except:\n",
    "            continue\n",
    "        if isinstance(shp, tuple) and len(shp) == 4:\n",
    "            return lyr.name\n",
    "    return m.layers[1].name  # fallback\n",
    "\n",
    "LAST_FEAT = get_last_4d_layer(model)\n",
    "print(\"Grad-CAM feature layer:\", LAST_FEAT)\n",
    "\n",
    "@tf.function\n",
    "def _gradcam_core(img_batch, target_idx, feat_model, head_model):\n",
    "    with tf.GradientTape() as tape:\n",
    "        feats = feat_model(img_batch)                       # (B,h,w,c)\n",
    "        tape.watch(feats)\n",
    "        logits = head_model(feats, training=False)          # (B,C)\n",
    "        cls = logits[:, target_idx]\n",
    "    grads = tape.gradient(cls, feats)                       # (B,h,w,c)\n",
    "    w = tf.reduce_mean(grads, axis=(1,2), keepdims=True)    # (B,1,1,c)\n",
    "    cam = tf.nn.relu(tf.reduce_sum(w * feats, axis=-1))     # (B,h,w)\n",
    "    # normalize per-image\n",
    "    cam_min = tf.reduce_min(cam, axis=(1,2), keepdims=True)\n",
    "    cam_max = tf.reduce_max(cam, axis=(1,2), keepdims=True)\n",
    "    cam = (cam - cam_min) / (cam_max - cam_min + 1e-8)\n",
    "    return cam\n",
    "\n",
    "# Split model into feature extractor + head (after LAST_FEAT)\n",
    "def split_feature_and_head(m, last_feat_name=LAST_FEAT):\n",
    "    feat_layer = m.get_layer(last_feat_name)\n",
    "    feat_model = tf.keras.Model(m.input, feat_layer.output)              # input -> conv feat\n",
    "    # build head: from conv feat to logits, reusing layers after feat layer\n",
    "    idx = [i for (i, l) in enumerate(m.layers) if l.name == last_feat_name][0]\n",
    "    x = tf.keras.Input(shape=feat_layer.output_shape[1:])\n",
    "    y = x\n",
    "    for lyr in m.layers[idx+1:]:\n",
    "        y = lyr(y)\n",
    "    head_model = tf.keras.Model(x, y)\n",
    "    return feat_model, head_model\n",
    "\n",
    "feat_model, head_model = split_feature_and_head(model, LAST_FEAT)\n",
    "\n",
    "def gradcam_heatmap(img_raw, target_idx):\n",
    "    x = convnext_preprocess(img_raw[None, ...])\n",
    "    cam_small = _gradcam_core(x, tf.constant(target_idx, tf.int32), feat_model, head_model)[0] # (h,w)\n",
    "    cam = tf.image.resize(cam_small[..., None], IMG_SIZE)[...,0].numpy()\n",
    "    return np.clip(cam, 0, 1)\n",
    "\n",
    "def overlay_heatmap(img_raw, cam, alpha=0.45):\n",
    "    img = (img_raw.numpy() / 255.0)\n",
    "    cmap = plt.get_cmap('jet')\n",
    "    heat = cmap(cam)[..., :3]\n",
    "    out = (1 - alpha) * img + alpha * heat\n",
    "    return np.clip(out, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:37:31.737659Z",
     "iopub.status.busy": "2025-10-10T15:37:31.737353Z",
     "iopub.status.idle": "2025-10-10T15:37:31.750908Z",
     "shell.execute_reply": "2025-10-10T15:37:31.750205Z",
     "shell.execute_reply.started": "2025-10-10T15:37:31.737637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Backbone (ConvNeXt) as feature extractor\n",
    "backbone = model.get_layer(\"convnext_tiny\")\n",
    "\n",
    "x_in = tf.keras.Input(shape=(224, 224, 3))     # plain input tensor\n",
    "feat_out = backbone(x_in)                      # (None, 7, 7, 768)\n",
    "feat_model = tf.keras.Model(x_in, feat_out, name=\"feat_model\")\n",
    "\n",
    "# Head model: GAP -> BN -> Dropout -> Dense\n",
    "f_in = tf.keras.Input(shape=backbone.output.shape[1:])  # (7, 7, 768)\n",
    "gap  = model.get_layer(\"global_average_pooling2d_15\")(f_in)\n",
    "bn   = model.get_layer(\"batch_normalization_15\")(gap)\n",
    "drop = model.get_layer(\"dropout_15\")(bn)\n",
    "out  = model.get_layer(\"dense_15\")(drop)\n",
    "head_model = tf.keras.Model(f_in, out, name=\"head_model\")\n",
    "\n",
    "print(\"✅ Rebuilt feat_model and head_model with fresh inputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:37:35.426216Z",
     "iopub.status.busy": "2025-10-10T15:37:35.425507Z",
     "iopub.status.idle": "2025-10-10T15:37:35.430693Z",
     "shell.execute_reply": "2025-10-10T15:37:35.430051Z",
     "shell.execute_reply.started": "2025-10-10T15:37:35.426192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _gradcam_core(img_batch, target_idx, feat_model, head_model):\n",
    "    with tf.GradientTape() as tape:\n",
    "        feats = feat_model(img_batch, training=False)   # (B, 7, 7, 768)\n",
    "        tape.watch(feats)\n",
    "        logits = head_model(feats, training=False)      # (B, num_classes)\n",
    "        loss = logits[:, target_idx]\n",
    "\n",
    "    grads = tape.gradient(loss, feats)                  # (B,7,7,768)\n",
    "    pooled = tf.reduce_mean(grads, axis=(1,2), keepdims=True)\n",
    "    cam = tf.reduce_sum(feats * pooled, axis=-1)        # (B,7,7)\n",
    "    return cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:37:38.361125Z",
     "iopub.status.busy": "2025-10-10T15:37:38.360236Z",
     "iopub.status.idle": "2025-10-10T15:37:38.366014Z",
     "shell.execute_reply": "2025-10-10T15:37:38.365274Z",
     "shell.execute_reply.started": "2025-10-10T15:37:38.361093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def gradcam_heatmap(img_raw, target_idx):\n",
    "    x = convnext_preprocess(img_raw[None, ...])   # preprocess like training\n",
    "    cam_small = _gradcam_core(x, target_idx, feat_model, head_model)[0]\n",
    "    cam = tf.image.resize(cam_small[..., None], IMG_SIZE)[..., 0].numpy()\n",
    "    return np.maximum(cam, 0) / (cam.max() + 1e-8)   # normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:37:40.726374Z",
     "iopub.status.busy": "2025-10-10T15:37:40.726070Z",
     "iopub.status.idle": "2025-10-10T15:37:40.731383Z",
     "shell.execute_reply": "2025-10-10T15:37:40.730485Z",
     "shell.execute_reply.started": "2025-10-10T15:37:40.726352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def overlay_heatmap(img_raw, cam, alpha=0.4):\n",
    "    heatmap = plt.cm.jet(cam)[..., :3] * 255.0\n",
    "    overlay = (1 - alpha) * img_raw + alpha * heatmap\n",
    "    return np.clip(overlay, 0, 255).astype(\"uint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:37:44.268849Z",
     "iopub.status.busy": "2025-10-10T15:37:44.268227Z",
     "iopub.status.idle": "2025-10-10T15:37:50.329137Z",
     "shell.execute_reply": "2025-10-10T15:37:50.328405Z",
     "shell.execute_reply.started": "2025-10-10T15:37:44.268826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==== One-shot Grad-CAM overlay (self-contained) ====\n",
    "\n",
    "import numpy as np, tensorflow as tf, matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\n",
    "\n",
    "# ----- 1) Pick a validation sample\n",
    "FOLD_ID = 4  # change if needed\n",
    "val_paths  = folds_data[FOLD_ID]['val_paths']\n",
    "val_labels = folds_data[FOLD_ID]['val_labels']\n",
    "\n",
    "idx = 0\n",
    "path = val_paths[idx]\n",
    "true_id = int(val_labels[idx])\n",
    "true_name = class_names[true_id]\n",
    "\n",
    "# ----- 2) Load raw resized image (float32 [0..255])\n",
    "def load_img_for_convnext(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)  # switch to decode_png if needed\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    return tf.cast(img, tf.float32)\n",
    "\n",
    "img_raw = load_img_for_convnext(path)\n",
    "\n",
    "# ----- 3) Predict softmax\n",
    "def predict_softmax(img_raw_tf):\n",
    "    x = convnext_preprocess(img_raw_tf[None, ...])\n",
    "    p = model(x, training=False).numpy()[0]\n",
    "    pred_id = int(np.argmax(p))\n",
    "    conf = float(np.max(p))\n",
    "    ent  = float(-np.sum(p * np.log(p + 1e-12)))\n",
    "    return pred_id, conf, ent, p\n",
    "\n",
    "pred_id, conf, ent, p = predict_softmax(img_raw)\n",
    "\n",
    "# ----- 4) Build feature & head models (fresh Inputs to avoid graph KeyErrors)\n",
    "# find last 4D feature layer (ConvNeXt backbone)\n",
    "def get_last_4d_layer(m):\n",
    "    for lyr in reversed(m.layers):\n",
    "        try:\n",
    "            shp = lyr.output_shape\n",
    "            if isinstance(shp, tuple) and len(shp) == 4:\n",
    "                return lyr.name\n",
    "        except:\n",
    "            pass\n",
    "    raise RuntimeError(\"No 4D feature layer found.\")\n",
    "LAST_FEAT = get_last_4d_layer(model)  # usually 'convnext_tiny'\n",
    "\n",
    "# feature extractor: raw -> preprocess -> backbone features\n",
    "x_in = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "x_proc = convnext_preprocess(x_in)\n",
    "feat_out = model.get_layer(LAST_FEAT)(x_proc)\n",
    "feat_model = tf.keras.Model(x_in, feat_out, name=\"feat_model_gc\")\n",
    "\n",
    "# head: replay layers after LAST_FEAT\n",
    "idx_last = [i for i,l in enumerate(model.layers) if l.name == LAST_FEAT][0]\n",
    "f_in = tf.keras.Input(shape=feat_out.shape[1:])\n",
    "y = f_in\n",
    "for lyr in model.layers[idx_last+1:]:\n",
    "    y = lyr(y)\n",
    "head_model = tf.keras.Model(f_in, y, name=\"head_model_gc\")\n",
    "\n",
    "# ----- 5) Grad-CAM\n",
    "def gradcam_heatmap(img_raw_tf, target_idx):\n",
    "    with tf.GradientTape() as tape:\n",
    "        feats = feat_model(img_raw_tf[None, ...], training=False)   # (1,h,w,c)\n",
    "        tape.watch(feats)\n",
    "        logits = head_model(feats, training=False)                  # (1,C)\n",
    "        loss = logits[:, int(target_idx)]\n",
    "    grads = tape.gradient(loss, feats)                               # (1,h,w,c)\n",
    "    w = tf.reduce_mean(grads, axis=(1,2), keepdims=True)             # (1,1,1,c)\n",
    "    cam_small = tf.nn.relu(tf.reduce_sum(w * feats, axis=-1))        # (1,h,w)\n",
    "    cam = tf.image.resize(cam_small[..., None], IMG_SIZE)[0, ..., 0].numpy()\n",
    "    cam = np.maximum(cam, 0) / (cam.max() + 1e-8)\n",
    "    return cam\n",
    "\n",
    "cam = gradcam_heatmap(img_raw, pred_id)\n",
    "\n",
    "# ----- 6) Overlay utility\n",
    "def overlay_heatmap(img_raw_tf, cam, alpha=0.45):\n",
    "    img = (img_raw_tf.numpy() / 255.0)  # (H,W,3) in [0,1]\n",
    "    heat = plt.get_cmap(\"jet\")(cam)[..., :3]\n",
    "    out = (1 - alpha) * img + alpha * heat\n",
    "    return np.clip(out * 255.0, 0, 255).astype(\"uint8\")\n",
    "\n",
    "overlay = overlay_heatmap(img_raw, cam, alpha=0.45)\n",
    "\n",
    "# ----- 7) Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img_raw.numpy().astype(\"uint8\"))\n",
    "plt.title(f\"Raw | GT={true_name} | Pred={class_names[pred_id]} ({conf:.2f})\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(overlay)\n",
    "plt.title(\"Grad-CAM Overlay\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:38:01.326508Z",
     "iopub.status.busy": "2025-10-10T15:38:01.326236Z",
     "iopub.status.idle": "2025-10-10T15:38:01.331571Z",
     "shell.execute_reply": "2025-10-10T15:38:01.330934Z",
     "shell.execute_reply.started": "2025-10-10T15:38:01.326488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Metrics & plotting imports + safe setup ---\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Ensure output dir exists\n",
    "OUT_DIR = OUT_DIR if 'OUT_DIR' in globals() else \"postsegxai_metrics\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Sanity: N_CLASSES known\n",
    "assert 'N_CLASSES' in globals(), \"N_CLASSES not defined.\"\n",
    "assert 'class_names' in globals(), \"class_names not defined.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:38:04.710766Z",
     "iopub.status.busy": "2025-10-10T15:38:04.710284Z",
     "iopub.status.idle": "2025-10-10T16:14:29.688850Z",
     "shell.execute_reply": "2025-10-10T16:14:29.688206Z",
     "shell.execute_reply.started": "2025-10-10T15:38:04.710745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Predict entire validation fold\n",
    "y_true, y_pred = [], []\n",
    "y_prob = np.zeros((len(val_paths), N_CLASSES), dtype=np.float32)\n",
    "\n",
    "for i, path in enumerate(val_paths):\n",
    "    img_raw = load_img_for_convnext(path)\n",
    "    pred, conf, ent, p = predict_softmax(img_raw)\n",
    "    y_true.append(val_labels[i])\n",
    "    y_pred.append(pred)\n",
    "    y_prob[i] = p\n",
    "\n",
    "y_true = np.array(y_true, int)\n",
    "y_pred = np.array(y_pred, int)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(N_CLASSES)))\n",
    "cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm_norm, vmin=0, vmax=1)\n",
    "plt.title(\"Normalized Confusion Matrix (Val)\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.xticks(range(N_CLASSES), class_names, rotation=45, ha='right')\n",
    "plt.yticks(range(N_CLASSES), class_names)\n",
    "for i in range(N_CLASSES):\n",
    "    for j in range(N_CLASSES):\n",
    "        plt.text(j, i, f\"{cm_norm[i,j]:.2f}\", ha='center', va='center', fontsize=8, color='white' if cm_norm[i,j]>0.5 else 'black')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix.png\"), dpi=140)\n",
    "plt.show()\n",
    "\n",
    "# Classification report (per-class precision/recall/F1)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T16:35:06.120944Z",
     "iopub.status.busy": "2025-10-10T16:35:06.120668Z",
     "iopub.status.idle": "2025-10-10T16:35:06.561324Z",
     "shell.execute_reply": "2025-10-10T16:35:06.560547Z",
     "shell.execute_reply.started": "2025-10-10T16:35:06.120924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- ROC / AUC (one-vs-rest, with guards) ---\n",
    "import os, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Ensure OUT_DIR exists\n",
    "OUT_DIR = OUT_DIR if 'OUT_DIR' in globals() else \"postsegxai_metrics\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Binarize labels for ROC\n",
    "Y_true_bin = label_binarize(y_true, classes=list(range(N_CLASSES)))  # (N, C)\n",
    "\n",
    "fpr, tpr, roc_auc = {}, {}, {}\n",
    "valid_classes = []\n",
    "\n",
    "# Per-class ROC (skip classes that are constant in this fold)\n",
    "for c in range(N_CLASSES):\n",
    "    y_c = Y_true_bin[:, c]\n",
    "    # roc_curve needs both 0 and 1 present\n",
    "    if y_c.max() == y_c.min():\n",
    "        # no positives or no negatives for this class in the fold\n",
    "        continue\n",
    "    fpr[c], tpr[c], _ = roc_curve(y_c, y_prob[:, c])\n",
    "    roc_auc[c] = auc(fpr[c], tpr[c])\n",
    "    valid_classes.append(c)\n",
    "\n",
    "# Micro ROC (only if there is at least one valid class)\n",
    "if len(valid_classes) > 0:\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_true_bin[:, valid_classes].ravel(),\n",
    "                                              y_prob[:, valid_classes].ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Macro ROC over valid classes\n",
    "if len(valid_classes) > 0:\n",
    "    all_fpr = np.unique(np.concatenate([fpr[c] for c in valid_classes]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for c in valid_classes:\n",
    "        mean_tpr += np.interp(all_fpr, fpr[c], tpr[c])\n",
    "    mean_tpr /= len(valid_classes)\n",
    "    roc_auc[\"macro\"] = auc(all_fpr, mean_tpr)\n",
    "else:\n",
    "    all_fpr, mean_tpr = np.array([0,1]), np.array([0,1])\n",
    "    roc_auc[\"macro\"] = np.nan\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6.5, 5.5))\n",
    "for c in valid_classes:\n",
    "    plt.plot(fpr[c], tpr[c], lw=1, label=f\"{class_names[c]} (AUC={roc_auc[c]:.3f})\")\n",
    "if \"micro\" in fpr:\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], lw=2, linestyle=\"--\",\n",
    "             label=f\"micro (AUC={roc_auc['micro']:.3f})\")\n",
    "plt.plot([0,1],[0,1], \"k--\", lw=1)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC (Val)  |  macro AUC={roc_auc['macro']:.3f}\" if not np.isnan(roc_auc[\"macro\"]) else \"ROC (Val)\")\n",
    "plt.legend(fontsize=7, loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curves.png\"), dpi=140)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T16:35:12.721730Z",
     "iopub.status.busy": "2025-10-10T16:35:12.720977Z",
     "iopub.status.idle": "2025-10-10T16:35:13.075930Z",
     "shell.execute_reply": "2025-10-10T16:35:13.075170Z",
     "shell.execute_reply.started": "2025-10-10T16:35:12.721707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Reliability / calibration: compare confidence vs accuracy\n",
    "conf = y_prob.max(axis=1)                 # predicted confidence\n",
    "correct = (y_pred == y_true).astype(int)  # 1/0 per sample\n",
    "\n",
    "bins = np.linspace(0.0, 1.0, 11)  # 10 bins\n",
    "bin_ids = np.digitize(conf, bins) - 1\n",
    "bin_acc, bin_conf, bin_count = [], [], []\n",
    "\n",
    "ECE = 0.0\n",
    "N = len(conf)\n",
    "for b in range(len(bins)-1):\n",
    "    idx = (bin_ids == b)\n",
    "    if np.any(idx):\n",
    "        acc_b = correct[idx].mean()\n",
    "        conf_b = conf[idx].mean()\n",
    "        cnt_b = idx.sum()\n",
    "        ECE += (cnt_b / N) * abs(acc_b - conf_b)\n",
    "        bin_acc.append(acc_b); bin_conf.append(conf_b); bin_count.append(cnt_b)\n",
    "    else:\n",
    "        bin_acc.append(np.nan); bin_conf.append(np.nan); bin_count.append(0)\n",
    "\n",
    "# Plot\n",
    "centers = 0.5*(bins[:-1] + bins[1:])\n",
    "plt.figure(figsize=(5.2,5))\n",
    "plt.plot([0,1],[0,1],'k--',label='Perfect calibration')\n",
    "plt.scatter(centers, bin_acc, s=np.array(bin_count)*2+5, label='Observed acc')\n",
    "plt.plot(centers, bin_conf, label='Mean conf', alpha=0.7)\n",
    "plt.xlim(0,1); plt.ylim(0,1)\n",
    "plt.xlabel(\"Confidence\"); plt.ylabel(\"Accuracy\")\n",
    "plt.title(f\"Reliability Diagram (ECE={ECE:.3f})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"reliability_diagram.png\"), dpi=140)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Expected Calibration Error (ECE): {ECE:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8298724,
     "sourceId": 13100888,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
