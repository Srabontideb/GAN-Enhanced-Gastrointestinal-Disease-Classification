{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13100888,"sourceType":"datasetVersion","datasetId":8298724}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T15:04:56.546365Z","iopub.execute_input":"2025-10-03T15:04:56.546696Z","iopub.status.idle":"2025-10-03T15:05:03.048234Z","shell.execute_reply.started":"2025-10-03T15:04:56.546665Z","shell.execute_reply":"2025-10-03T15:05:03.047522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # 0 = all logs, 1 = filter INFO, 2 = filter WARNING, 3 = filter ERROR\nimport tensorflow as tf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T11:58:21.227190Z","iopub.execute_input":"2025-10-10T11:58:21.227964Z","iopub.status.idle":"2025-10-10T11:58:21.231734Z","shell.execute_reply.started":"2025-10-10T11:58:21.227937Z","shell.execute_reply":"2025-10-10T11:58:21.230965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, glob\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import shuffle\n\ndata_dir = \"/kaggle/input/kvasir-final-preprocessed-data/kvasir_bilateral_filtered\"\n\nimage_paths = []\nlabels = []\n\n# Get class folders\nclass_names = sorted(os.listdir(data_dir))\nclass_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n\nfor cls_name in class_names:\n    cls_folder = os.path.join(data_dir, cls_name)\n    for img_path in glob.glob(os.path.join(cls_folder, '*.*')):\n        image_paths.append(img_path)\n        labels.append(class_to_idx[cls_name])\n\nimage_paths, labels = shuffle(image_paths, labels, random_state=42)\nX = np.array(image_paths)\ny = np.array(labels)\n\nprint(f\"Total images: {len(X)}; Classes: {class_names}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T11:58:24.972063Z","iopub.execute_input":"2025-10-10T11:58:24.972334Z","iopub.status.idle":"2025-10-10T11:58:25.874742Z","shell.execute_reply.started":"2025-10-10T11:58:24.972314Z","shell.execute_reply":"2025-10-10T11:58:25.874078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================\n# 1) Train/Val/Test split (stratified)\n# ============================\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nimport numpy as np, pandas as pd, time, os\nfrom pathlib import Path\n\nRANDOM_STATE = 42\nTEST_SIZE    = 0.20      # <- change if you want (e.g., 0.1 or 0.15)\nN_SPLITS     = 5         # k for CV\n\n# X = np.array(image_paths); y = np.array(labels)  # <- you already have these\n\n# Hold out the test set FIRST (never touched during CV/early stopping)\nX_trainval, X_test, y_trainval, y_test = train_test_split(\n    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n)\n\n# Safety checks (no overlap)\nassert set(X_trainval).isdisjoint(set(X_test)), \"Leak: train/val and test overlap!\"\nassert len(X_trainval) + len(X_test) == len(X)\n\nprint(f\"Train+Val: {len(X_trainval)}   Test: {len(X_test)}\")\n\n# ============================\n# 2) Stratified K-Fold on TRAIN+VAL ONLY\n# ============================\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n\nfolds_data = []\nfor fold, (tr_idx, val_idx) in enumerate(skf.split(X_trainval, y_trainval), start=1):\n    folds_data.append({\n        \"fold\": fold,\n        \"train_paths\": X_trainval[tr_idx],\n        \"train_labels\": y_trainval[tr_idx],\n        \"val_paths\":   X_trainval[val_idx],\n        \"val_labels\":  y_trainval[val_idx],\n    })\n    print(f\"Fold {fold}: train={len(tr_idx)}, val={len(val_idx)}\")\n\n# ============================\n# 3) Save splits for reuse (Quick Save will capture these)\n# ============================\nOUT = Path(\"/kaggle/working\")/f\"splits_{time.strftime('%Y%m%d_%H%M')}\"\nOUT.mkdir(parents=True, exist_ok=True)\n\n# Test set (held-out)\npd.DataFrame({\"filepath\": X_test, \"label\": y_test}).to_csv(OUT/\"test.csv\", index=False)\n\n# Train/Val folds\npd.DataFrame({\"filepath\": X_trainval, \"label\": y_trainval}).to_csv(OUT/\"trainval.csv\", index=False)\nfor f in folds_data:\n    fold = f[\"fold\"]\n    pd.DataFrame({\"filepath\": f[\"train_paths\"], \"label\": f[\"train_labels\"]}).to_csv(OUT/f\"train_fold{fold}.csv\", index=False)\n    pd.DataFrame({\"filepath\": f[\"val_paths\"],   \"label\": f[\"val_labels\"]}).to_csv(OUT/f\"val_fold{fold}.csv\",   index=False)\n\nprint(\"Saved splits to:\", OUT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T11:58:29.856052Z","iopub.execute_input":"2025-10-10T11:58:29.856758Z","iopub.status.idle":"2025-10-10T11:58:30.050810Z","shell.execute_reply.started":"2025-10-10T11:58:29.856734Z","shell.execute_reply":"2025-10-10T11:58:30.050033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tf.config.list_physical_devices('GPU'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T11:58:34.657251Z","iopub.execute_input":"2025-10-10T11:58:34.657788Z","iopub.status.idle":"2025-10-10T11:58:35.375840Z","shell.execute_reply.started":"2025-10-10T11:58:34.657766Z","shell.execute_reply":"2025-10-10T11:58:35.374990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\nIMG_SIZE = (224, 224)  # adjust as needed\nBATCH_SIZE = 32\n\ndef decode_img(file_path, label):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)  # or decode_png if png\n    img = tf.image.resize(img, IMG_SIZE)\n    img = tf.cast(img, tf.float32) / 255.0\n    return img, label\n\ndef make_tf_dataset(file_paths, labels, batch_size=BATCH_SIZE, shuffle_data=True):\n    ds = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n    ds = ds.map(decode_img, num_parallel_calls=tf.data.AUTOTUNE)\n    if shuffle_data:\n        ds = ds.shuffle(buffer_size=len(file_paths))\n    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return ds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T11:58:38.322232Z","iopub.execute_input":"2025-10-10T11:58:38.322794Z","iopub.status.idle":"2025-10-10T11:58:38.328097Z","shell.execute_reply.started":"2025-10-10T11:58:38.322773Z","shell.execute_reply":"2025-10-10T11:58:38.327318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T11:58:41.273258Z","iopub.execute_input":"2025-10-10T11:58:41.273529Z","iopub.status.idle":"2025-10-10T11:58:41.335333Z","shell.execute_reply.started":"2025-10-10T11:58:41.273508Z","shell.execute_reply":"2025-10-10T11:58:41.334572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 0) Imports\n# =========================\nimport os, math, numpy as np, tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, losses, metrics\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\nfrom tensorflow.keras.mixed_precision import set_global_policy\n\n# Optional: mixed precision for speed on P100 (usually beneficial)\nset_global_policy(\"mixed_float16\")\n\n# Keras ConvNeXt (requires TF/Keras >= 2.11)\nfrom tensorflow.keras.applications import ConvNeXtTiny\nfrom tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\n\n# =========================\n# 1) Config\n# =========================\nIMG_SIZE = (224, 224)         # ConvNeXt default\nBATCH_SIZE = 32\nEPOCHS_WARMUP = 3             # freeze backbone first\nEPOCHS_FINETUNE = 12          # then unfreeze\nLR_WARMUP = 1e-3\nLR_FINETUNE = 5e-4\nAUTOTUNE = tf.data.AUTOTUNE\nN_FOLDS = len(folds_data)     # should be 5 for your setup\nN_CLASSES = len(class_names)\n\n# =========================\n# 2) Data pipeline\n#    (ConvNeXt expects its own preprocessing; we keep images in [0..255] float and apply convnext_preprocess)\n# =========================\ndef decode_img_for_convnext(file_path, label):\n    img = tf.io.read_file(file_path)\n    # change decode_* if your files are PNG\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, IMG_SIZE)\n    # keep as float32 in [0..255] for convnext_preprocess\n    img = tf.cast(img, tf.float32)\n    return img, label\n\n# Light on-the-fly augmentation (you can tune as needed)\ndata_augment = tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.05),\n    layers.RandomContrast(0.1),\n], name=\"augment\")\n\n@tf.function\ndef apply_preprocess(x, y):\n    # ConvNeXt preprocess: channel-wise normalize to ImageNet statistics internally\n    x = convnext_preprocess(x)   # returns float32\n    return x, y\n\ndef make_tf_dataset_convnext(file_paths, labels, training=True):\n    ds = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n    ds = ds.shuffle(len(file_paths), reshuffle_each_iteration=True) if training else ds\n    ds = ds.map(decode_img_for_convnext, num_parallel_calls=AUTOTUNE)\n    if training:\n        ds = ds.map(lambda x, y: (data_augment(x, training=True), y), num_parallel_calls=AUTOTUNE)\n    ds = ds.map(apply_preprocess, num_parallel_calls=AUTOTUNE)\n    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    return ds\n\n# =========================\n# 3) ConvNeXt-Tiny model builder\n# =========================\ndef build_convnext_tiny(num_classes=N_CLASSES, image_size=IMG_SIZE, trainable_backbone=False):\n    # Backbone\n    backbone = ConvNeXtTiny(\n        include_top=False,\n        weights=\"imagenet\",\n        input_shape=(image_size[0], image_size[1], 3),\n        pooling=None\n    )\n    backbone.trainable = trainable_backbone  # warmup: False; finetune: True\n\n    inputs = layers.Input(shape=(image_size[0], image_size[1], 3))\n    # NOTE: we already applied convnext_preprocess in the dataset, so we pass inputs directly.\n    x = backbone(inputs, training=False)\n\n    # Head (recommend GAP + BN + dropout + Dense)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    # Final classifier\n    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)  # force float32 for numeric stability\n\n    model = models.Model(inputs, outputs, name=\"ConvNeXtTiny_Kvasir\")\n    return model\n\n# =========================\n# 4) Training utilities\n# =========================\ndef compile_model(model, lr):\n    opt = optimizers.Adam(learning_rate=lr)\n    model.compile(\n        optimizer=opt,\n        loss=losses.SparseCategoricalCrossentropy(),\n        metrics=[metrics.SparseCategoricalAccuracy(name=\"acc\")]\n    )\n\ndef get_callbacks(fold_id):\n    os.makedirs(\"convnext_runs\", exist_ok=True)\n    return [\n        EarlyStopping(monitor=\"val_acc\", patience=6, mode=\"max\", restore_best_weights=True, verbose=1),\n        ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.5, patience=3, verbose=1, min_lr=1e-6),\n        ModelCheckpoint(\n            filepath=f\"convnext_runs/best_convnextT_fold{fold_id}.h5\",\n            monitor=\"val_acc\", mode=\"max\", save_best_only=True, verbose=1\n        ),\n        CSVLogger(f\"convnext_runs/training_log_fold{fold_id}.csv\", append=False)\n    ]\n\n# =========================\n# 5) 5-Fold training (warmup + finetune)\n# =========================\nfold_metrics = []\n\nfor fold_id in range(N_FOLDS):\n    print(f\"\\n================= FOLD {fold_id+1}/{N_FOLDS} =================\")\n\n    # Build datasets\n    train_paths = folds_data[fold_id]['train_paths']\n    train_labels = folds_data[fold_id]['train_labels']\n    val_paths   = folds_data[fold_id]['val_paths']\n    val_labels  = folds_data[fold_id]['val_labels']\n\n    train_ds = make_tf_dataset_convnext(train_paths, train_labels, training=True)\n    val_ds   = make_tf_dataset_convnext(val_paths,   val_labels,   training=False)\n\n    # 5.1 Warmup: freeze backbone\n    model = build_convnext_tiny(trainable_backbone=False)\n    compile_model(model, lr=LR_WARMUP)\n    cb = get_callbacks(fold_id)\n\n    print(\"\\n---- Warmup (frozen backbone) ----\")\n    history_warmup = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=EPOCHS_WARMUP,\n        callbacks=cb,\n        verbose=1\n    )\n\n    # 5.2 Fine-tune: unfreeze backbone\n    model.get_layer(index=1).trainable = True  # backbone is the 2nd layer (index 1)\n    # (Optional) fine-tune last N blocks only:\n    # for l in model.get_layer(index=1).layers[:-20]:  # keep last ~20 layers trainable\n    #     l.trainable = False\n\n    compile_model(model, lr=LR_FINETUNE)\n    print(\"\\n---- Fine-tuning (unfrozen backbone) ----\")\n    history_ft = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=EPOCHS_FINETUNE,\n        callbacks=cb,\n        verbose=1\n    )\n\n    # Evaluate best weights\n    eval_res = model.evaluate(val_ds, verbose=0)\n    # eval_res -> [loss, acc]\n    fold_metrics.append({\"fold\": fold_id+1, \"val_loss\": float(eval_res[0]), \"val_acc\": float(eval_res[1])})\n    print(f\"FOLD {fold_id+1} → val_loss: {eval_res[0]:.4f} | val_acc: {eval_res[1]:.4f}\")\n\n# =========================\n# 6) Summary across folds\n# =========================\nval_accs  = [m[\"val_acc\"]  for m in fold_metrics]\nval_losses= [m[\"val_loss\"] for m in fold_metrics]\n\nprint(\"\\n========= 5-Fold Summary (ConvNeXt-T) =========\")\nfor m in fold_metrics:\n    print(f\"Fold {m['fold']}:  val_acc={m['val_acc']:.4f},  val_loss={m['val_loss']:.4f}\")\n\nprint(f\"\\nMean val_acc: {np.mean(val_accs):.4f}  (± {np.std(val_accs):.4f})\")\nprint(f\"Mean val_loss: {np.mean(val_losses):.4f} (± {np.std(val_losses):.4f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T12:32:53.748687Z","iopub.execute_input":"2025-10-09T12:32:53.749255Z","iopub.status.idle":"2025-10-09T14:00:56.183990Z","shell.execute_reply.started":"2025-10-09T12:32:53.749233Z","shell.execute_reply":"2025-10-09T14:00:56.183344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inline visualization of training/validation curves from CSVLogger outputs\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nfrom pathlib import Path\n\nLOG_DIR = Path(\"/kaggle/working/convnext_runs\")\nN_FOLDS = 5  # change if needed\n\ndef pick_cols(df):\n    m = {c.lower().strip(): c for c in df.columns}\n    acc = m.get(\"acc\") or m.get(\"accuracy\") or m.get(\"sparse_categorical_accuracy\") or m.get(\"categorical_accuracy\")\n    val_acc = m.get(\"val_acc\") or m.get(\"val_accuracy\") or m.get(\"val_sparse_categorical_accuracy\") or m.get(\"val_categorical_accuracy\")\n    loss = m.get(\"loss\")\n    val_loss = m.get(\"val_loss\")\n    return acc, val_acc, loss, val_loss\n\n# --- Per-fold panels ---\nfor f in range(N_FOLDS):\n    csvp = LOG_DIR / f\"training_log_fold{f}.csv\"\n    if not csvp.exists():\n        print(f\"[skip] {csvp} not found\"); \n        continue\n    df = pd.read_csv(csvp)\n    acc, val_acc, loss, val_loss = pick_cols(df)\n    epochs = np.arange(1, len(df)+1)\n\n    # Accuracy\n    plt.figure(figsize=(7,4))\n    if acc:     plt.plot(epochs, df[acc], label=\"train acc\")\n    if val_acc: plt.plot(epochs, df[val_acc], label=\"val acc\")\n    plt.title(f\"Fold {f} • Accuracy\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.grid(True, linewidth=0.3); plt.legend()\n    plt.show()\n\n    # Loss\n    plt.figure(figsize=(7,4))\n    if loss:     plt.plot(epochs, df[loss], label=\"train loss\")\n    if val_loss: plt.plot(epochs, df[val_loss], label=\"val loss\")\n    plt.title(f\"Fold {f} • Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.grid(True, linewidth=0.3); plt.legend()\n    plt.show()\n\n# --- All-folds comparison (validation curves only) ---\nplt.figure(figsize=(8,5))\nfor f in range(N_FOLDS):\n    csvp = LOG_DIR / f\"training_log_fold{f}.csv\"\n    if not csvp.exists(): continue\n    df = pd.read_csv(csvp)\n    _, val_acc, _, val_loss = pick_cols(df)\n    if val_acc: plt.plot(np.arange(1, len(df)+1), df[val_acc], label=f\"Fold {f}\")\nplt.title(\"Validation Accuracy — All Folds\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Accuracy\"); plt.grid(True, linewidth=0.3); plt.legend()\nplt.show()\n\nplt.figure(figsize=(8,5))\nfor f in range(N_FOLDS):\n    csvp = LOG_DIR / f\"training_log_fold{f}.csv\"\n    if not csvp.exists(): continue\n    df = pd.read_csv(csvp)\n    _, _, _, val_loss = pick_cols(df)\n    if val_loss: plt.plot(np.arange(1, len(df)+1), df[val_loss], label=f\"Fold {f}\")\nplt.title(\"Validation Loss — All Folds\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Loss\"); plt.grid(True, linewidth=0.3); plt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T14:11:34.046604Z","iopub.execute_input":"2025-10-09T14:11:34.046841Z","iopub.status.idle":"2025-10-09T14:11:36.007085Z","shell.execute_reply.started":"2025-10-09T14:11:34.046825Z","shell.execute_reply":"2025-10-09T14:11:36.006422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============== Robust Grad-CAM (manual forward; heatmap only) ===============\nimport os, glob, random, numpy as np, tensorflow as tf, cv2\nfrom pathlib import Path\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import ConvNeXtTiny\nfrom tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\n\n# --- CONFIG ---\nDATA_ROOT = \"/kaggle/input/kvasir-final-preprocessed-data/kvasir_bilateral_filtered\"\nWEIGHTS   = Path(\"/kaggle/working/convnext_runs/best_convnextT_fold4.h5\")  # change fold if you want\nIMG_SIZE  = (224, 224)\nN_CLASSES = 8\nTARGET    = \"pred\"  # \"pred\" or an int class id\nOUT_DIR   = Path(\"/kaggle/working/gradcam_heatmaps\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# --- Build SAME head you trained and load weights ---\ndef build_model(nc=N_CLASSES, image_size=IMG_SIZE):\n    bb = ConvNeXtTiny(include_top=False, weights=None,\n                      input_shape=(image_size[0], image_size[1], 3), pooling=None)\n    inp = layers.Input(shape=(image_size[0], image_size[1], 3))\n    x = bb(inp, training=False)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    out = layers.Dense(nc, activation=\"softmax\", dtype=\"float32\")(x)\n    return models.Model(inp, out)\n\nmodel = build_model()\nmodel.load_weights(str(WEIGHTS))\n\n# --- Pick a real image (or set IMG_PATH yourself) ---\ndef pick_image(root):\n    exts = (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\",\"*.JPEG\")\n    c = []\n    for e in exts: c += glob.glob(os.path.join(root, \"**\", e), recursive=True)\n    assert c, f\"No images under {root}\"\n    return random.choice(c)\n\nIMG_PATH = pick_image(DATA_ROOT)\nprint(\"Using image:\", IMG_PATH)\n\n# --- Preprocess ---\ndef load_for_model(fp, size):\n    img = tf.io.read_file(fp)\n    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n    img = tf.image.resize(img, size)\n    x = tf.cast(img, tf.float32)          # [0..255]\n    x = convnext_preprocess(x)            # ConvNeXt preprocessing\n    return tf.expand_dims(x, 0), tf.cast(img, tf.uint8)\n\nx, img_uint8 = load_for_model(IMG_PATH, IMG_SIZE)\n\n# --- Helpers ---\ndef find_last_4d_layer_name(m):\n    # walk from end to start to get the deepest 4D (HWC) tensor layer\n    for lyr in reversed(m.layers):\n        shp = getattr(lyr, \"output_shape\", None)\n        if isinstance(shp, tuple) and len(shp) == 4:\n            return lyr.name\n    raise ValueError(\"No 4D conv feature layer found for Grad-CAM.\")\n\nLAST_NAME = find_last_4d_layer_name(model)\n\ndef gradcam_manual(m, x, class_idx, last_name, img_size):\n    \"\"\"\n    Manual forward through layers inside one GradientTape to avoid Keras-3 multi-output issues.\n    \"\"\"\n    with tf.GradientTape() as tape:\n        t = x\n        conv_out = None\n        for lyr in m.layers[1:]:  # skip InputLayer at index 0\n            t = lyr(t, training=False)\n            if lyr.name == last_name:\n                conv_out = t\n        preds = t                            # (1, C)\n        target = preds[:, class_idx]         # scalar per sample\n    assert conv_out is not None, \"Last conv feature not found during forward pass.\"\n    grads   = tape.gradient(target, conv_out)          # (1, h, w, C)\n    weights = tf.reduce_mean(grads, axis=(0,1,2))      # (C,)\n    cam     = tf.tensordot(conv_out[0], weights, axes=1)  # (h, w)\n    cam     = tf.nn.relu(cam)\n    cam     = cam / (tf.reduce_max(cam) + 1e-12)\n    cam     = tf.image.resize(cam[...,None], img_size)[...,0]\n    return cam.numpy(), preds.numpy()[0]\n\n# --- First pass to get predicted class (then CAM for it) ---\ncam_tmp, probs0 = gradcam_manual(model, x, class_idx=0, last_name=LAST_NAME, img_size=IMG_SIZE)\npred_id = int(np.argmax(probs0))\nclass_idx = pred_id if TARGET == \"pred\" else int(TARGET)\n\n# --- Final CAM for chosen class ---\ncam, probs = gradcam_manual(model, x, class_idx=class_idx, last_name=LAST_NAME, img_size=IMG_SIZE)\n\n# --- Save grayscale heatmap ---\nout_path = OUT_DIR / f\"cam_{Path(IMG_PATH).stem}_class{class_idx}.png\"\ncv2.imwrite(str(out_path), (np.clip(cam,0,1)*255).astype(np.uint8))\nprint(f\"Saved heatmap → {out_path}\")\nprint(\"Predicted class id:\", pred_id, \"| CAM for class id:\", class_idx)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T14:11:44.460120Z","iopub.execute_input":"2025-10-09T14:11:44.460409Z","iopub.status.idle":"2025-10-09T14:12:01.457964Z","shell.execute_reply.started":"2025-10-09T14:11:44.460389Z","shell.execute_reply":"2025-10-09T14:12:01.457290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Phase 5: PostSegXAI — setup\nimport os, math, numpy as np, tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\n# Save dirs\nFOLD_ID = 4           # <-- choose the fold you want to inspect\nOUT_DIR = f\"postsegxai_fold{FOLD_ID}\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# Convenience\nN_CLASSES = len(class_names)\nprint(\"Classes:\", class_names)\nprint(\"Fold:\", FOLD_ID)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T14:23:00.984397Z","iopub.execute_input":"2025-10-09T14:23:00.984667Z","iopub.status.idle":"2025-10-09T14:23:00.990094Z","shell.execute_reply.started":"2025-10-09T14:23:00.984646Z","shell.execute_reply":"2025-10-09T14:23:00.989376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# If build_convnext_tiny & convnext_preprocess are not in scope, re-import:\nfrom tensorflow.keras.applications import ConvNeXtTiny\nfrom tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\nfrom tensorflow.keras import layers, models\n\ndef build_convnext_tiny(num_classes=N_CLASSES, image_size=IMG_SIZE, trainable_backbone=True):\n    backbone = ConvNeXtTiny(\n        include_top=False,\n        weights=\"imagenet\",\n        input_shape=(image_size[0], image_size[1], 3),\n        pooling=None\n    )\n    backbone.trainable = trainable_backbone\n    inputs = layers.Input(shape=(image_size[0], image_size[1], 3))\n    x = backbone(inputs, training=False)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n    return models.Model(inputs, outputs, name=\"ConvNeXtTiny_Kvasir\")\n\n# Rebuild + load weights from training (works even if .h5 full-model load fails)\nmodel = build_convnext_tiny()\nmodel.load_weights(f\"convnext_runs/best_convnextT_fold{FOLD_ID}.h5\")\nprint(\"Model rebuilt + weights loaded.\")\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T14:23:03.851387Z","iopub.execute_input":"2025-10-09T14:23:03.852011Z","iopub.status.idle":"2025-10-09T14:23:05.197731Z","shell.execute_reply.started":"2025-10-09T14:23:03.851987Z","shell.execute_reply":"2025-10-09T14:23:05.196992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths for this fold\nval_paths = folds_data[FOLD_ID]['val_paths']\nval_labels = folds_data[FOLD_ID]['val_labels']\n\ndef load_img_for_convnext(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)    # change to decode_png if needed\n    img = tf.image.resize(img, IMG_SIZE)\n    img = tf.cast(img, tf.float32)                 # [0..255]\n    return img\n\ndef predict_softmax(img_raw):\n    x = convnext_preprocess(img_raw[None, ...])    # preprocess inside\n    p = model(x, training=False).numpy()[0]        # (C,)\n    pred = int(np.argmax(p))\n    conf = float(np.max(p))\n    # uncertainty via entropy (nats)\n    entropy = float(-np.sum(p * np.log(p + 1e-12)))\n    return pred, conf, entropy, p\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T14:23:13.966483Z","iopub.execute_input":"2025-10-09T14:23:13.966740Z","iopub.status.idle":"2025-10-09T14:23:13.972603Z","shell.execute_reply.started":"2025-10-09T14:23:13.966720Z","shell.execute_reply":"2025-10-09T14:23:13.971861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find last 4D feature layer automatically (should be 'convnext_tiny')\ndef get_last_4d_layer(m):\n    for lyr in reversed(m.layers):\n        try:\n            shp = lyr.output_shape\n        except:\n            continue\n        if isinstance(shp, tuple) and len(shp) == 4:\n            return lyr.name\n    return m.layers[1].name  # fallback\n\nLAST_FEAT = get_last_4d_layer(model)\nprint(\"Grad-CAM feature layer:\", LAST_FEAT)\n\n@tf.function\ndef _gradcam_core(img_batch, target_idx, feat_model, head_model):\n    with tf.GradientTape() as tape:\n        feats = feat_model(img_batch)                       # (B,h,w,c)\n        tape.watch(feats)\n        logits = head_model(feats, training=False)          # (B,C)\n        cls = logits[:, target_idx]\n    grads = tape.gradient(cls, feats)                       # (B,h,w,c)\n    w = tf.reduce_mean(grads, axis=(1,2), keepdims=True)    # (B,1,1,c)\n    cam = tf.nn.relu(tf.reduce_sum(w * feats, axis=-1))     # (B,h,w)\n    # normalize per-image\n    cam_min = tf.reduce_min(cam, axis=(1,2), keepdims=True)\n    cam_max = tf.reduce_max(cam, axis=(1,2), keepdims=True)\n    cam = (cam - cam_min) / (cam_max - cam_min + 1e-8)\n    return cam\n\n# Split model into feature extractor + head (after LAST_FEAT)\ndef split_feature_and_head(m, last_feat_name=LAST_FEAT):\n    feat_layer = m.get_layer(last_feat_name)\n    feat_model = tf.keras.Model(m.input, feat_layer.output)              # input -> conv feat\n    # build head: from conv feat to logits, reusing layers after feat layer\n    idx = [i for (i, l) in enumerate(m.layers) if l.name == last_feat_name][0]\n    x = tf.keras.Input(shape=feat_layer.output_shape[1:])\n    y = x\n    for lyr in m.layers[idx+1:]:\n        y = lyr(y)\n    head_model = tf.keras.Model(x, y)\n    return feat_model, head_model\n\nfeat_model, head_model = split_feature_and_head(model, LAST_FEAT)\n\ndef gradcam_heatmap(img_raw, target_idx):\n    x = convnext_preprocess(img_raw[None, ...])\n    cam_small = _gradcam_core(x, tf.constant(target_idx, tf.int32), feat_model, head_model)[0] # (h,w)\n    cam = tf.image.resize(cam_small[..., None], IMG_SIZE)[...,0].numpy()\n    return np.clip(cam, 0, 1)\n\ndef overlay_heatmap(img_raw, cam, alpha=0.45):\n    img = (img_raw.numpy() / 255.0)\n    cmap = plt.get_cmap('jet')\n    heat = cmap(cam)[..., :3]\n    out = (1 - alpha) * img + alpha * heat\n    return np.clip(out, 0, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T14:23:16.735406Z","iopub.execute_input":"2025-10-09T14:23:16.735948Z","iopub.status.idle":"2025-10-09T14:23:16.756283Z","shell.execute_reply.started":"2025-10-09T14:23:16.735925Z","shell.execute_reply":"2025-10-09T14:23:16.755568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Backbone (ConvNeXt) as feature extractor\nbackbone = model.get_layer(\"convnext_tiny\")\n\nx_in = tf.keras.Input(shape=(224, 224, 3))     # plain input tensor\nfeat_out = backbone(x_in)                      # (None, 7, 7, 768)\nfeat_model = tf.keras.Model(x_in, feat_out, name=\"feat_model\")\n\n# Head model: GAP -> BN -> Dropout -> Dense\nf_in = tf.keras.Input(shape=backbone.output.shape[1:])  # (7, 7, 768)\ngap  = model.get_layer(\"global_average_pooling2d_8\")(f_in)\nbn   = model.get_layer(\"batch_normalization_8\")(gap)\ndrop = model.get_layer(\"dropout_8\")(bn)\nout  = model.get_layer(\"dense_8\")(drop)\nhead_model = tf.keras.Model(f_in, out, name=\"head_model\")\n\nprint(\"✅ Rebuilt feat_model and head_model with fresh inputs\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T14:24:59.678802Z","iopub.execute_input":"2025-10-09T14:24:59.679359Z","iopub.status.idle":"2025-10-09T14:24:59.692295Z","shell.execute_reply.started":"2025-10-09T14:24:59.679333Z","shell.execute_reply":"2025-10-09T14:24:59.691548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _gradcam_core(img_batch, target_idx, feat_model, head_model):\n    with tf.GradientTape() as tape:\n        feats = feat_model(img_batch, training=False)   # (B, 7, 7, 768)\n        tape.watch(feats)\n        logits = head_model(feats, training=False)      # (B, num_classes)\n        loss = logits[:, target_idx]\n\n    grads = tape.gradient(loss, feats)                  # (B,7,7,768)\n    pooled = tf.reduce_mean(grads, axis=(1,2), keepdims=True)\n    cam = tf.reduce_sum(feats * pooled, axis=-1)        # (B,7,7)\n    return cam\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T14:25:03.067200Z","iopub.execute_input":"2025-10-09T14:25:03.067937Z","iopub.status.idle":"2025-10-09T14:25:03.073761Z","shell.execute_reply.started":"2025-10-09T14:25:03.067905Z","shell.execute_reply":"2025-10-09T14:25:03.072933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def gradcam_heatmap(img_raw, target_idx):\n    x = convnext_preprocess(img_raw[None, ...])   # preprocess like training\n    cam_small = _gradcam_core(x, target_idx, feat_model, head_model)[0]\n    cam = tf.image.resize(cam_small[..., None], IMG_SIZE)[..., 0].numpy()\n    return np.maximum(cam, 0) / (cam.max() + 1e-8)   # normalize\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T14:25:07.920728Z","iopub.execute_input":"2025-10-09T14:25:07.920989Z","iopub.status.idle":"2025-10-09T14:25:07.925487Z","shell.execute_reply.started":"2025-10-09T14:25:07.920968Z","shell.execute_reply":"2025-10-09T14:25:07.924748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def overlay_heatmap(img_raw, cam, alpha=0.4):\n    heatmap = plt.cm.jet(cam)[..., :3] * 255.0\n    overlay = (1 - alpha) * img_raw + alpha * heatmap\n    return np.clip(overlay, 0, 255).astype(\"uint8\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T14:25:09.675496Z","iopub.execute_input":"2025-10-09T14:25:09.675755Z","iopub.status.idle":"2025-10-09T14:25:09.679959Z","shell.execute_reply.started":"2025-10-09T14:25:09.675736Z","shell.execute_reply":"2025-10-09T14:25:09.679311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict entire validation fold\ny_true, y_pred = [], []\ny_prob = np.zeros((len(val_paths), N_CLASSES), dtype=np.float32)\n\nfor i, path in enumerate(val_paths):\n    img_raw = load_img_for_convnext(path)\n    pred, conf, ent, p = predict_softmax(img_raw)\n    y_true.append(val_labels[i])\n    y_pred.append(pred)\n    y_prob[i] = p\n\ny_true = np.array(y_true, int)\ny_pred = np.array(y_pred, int)\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred, labels=list(range(N_CLASSES)))\ncm_norm = cm / cm.sum(axis=1, keepdims=True)\n\nplt.figure(figsize=(6,5))\nplt.figure(figsize=(6,5))\nplt.imshow(cm_norm, vmin=0, vmax=1,cmap=\"viridis\")\nplt.title(\"Normalized Confusion Matrix (Val)\")\nplt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\nplt.xticks(range(N_CLASSES), class_names, rotation=45, ha='right')\nplt.yticks(range(N_CLASSES), class_names)\nfor i in range(N_CLASSES):\n    for j in range(N_CLASSES):\n        plt.text(j, i, f\"{cm_norm[i,j]:.2f}\", ha='center', va='center', fontsize=8, color='white' if cm_norm[i,j]>0.5 else 'black')\nplt.tight_layout()\nplt.savefig(os.path.join(OUT_DIR, \"confusion_matrix.png\"), dpi=140)\nplt.show()\n\n# Classification report (per-class precision/recall/F1)\nprint(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T14:33:26.862914Z","iopub.execute_input":"2025-10-09T14:33:26.863207Z","iopub.status.idle":"2025-10-09T15:09:18.210509Z","shell.execute_reply.started":"2025-10-09T14:33:26.863184Z","shell.execute_reply":"2025-10-09T15:09:18.209728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Binarize labels for ROC\nY_true_bin = label_binarize(y_true, classes=list(range(N_CLASSES)))  # shape (N, C)\n\nfpr, tpr, roc_auc = {}, {}, {}\nfor c in range(N_CLASSES):\n    fpr[c], tpr[c], _ = roc_curve(Y_true_bin[:, c], y_prob[:, c])\n    roc_auc[c] = auc(fpr[c], tpr[c])\n\n# Micro/macro\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_true_bin.ravel(), y_prob.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\nall_fpr = np.unique(np.concatenate([fpr[c] for c in range(N_CLASSES)]))\nmean_tpr = np.zeros_like(all_fpr)\nfor c in range(N_CLASSES):\n    mean_tpr += np.interp(all_fpr, fpr[c], tpr[c])\nmean_tpr /= N_CLASSES\nroc_auc[\"macro\"] = auc(all_fpr, mean_tpr)\n\nplt.figure(figsize=(6,5))\nfor c in range(N_CLASSES):\n    plt.plot(fpr[c], tpr[c], lw=1, label=f\"{class_names[c]} (AUC={roc_auc[c]:.3f})\")\nplt.plot(fpr[\"micro\"], tpr[\"micro\"], lw=2, label=f\"micro (AUC={roc_auc['micro']:.3f})\", linestyle='--')\nplt.plot([0,1],[0,1], 'k--', lw=1)\nplt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (Val)\")\nplt.legend(fontsize=7, loc=\"lower right\")\nplt.tight_layout()\nplt.savefig(os.path.join(OUT_DIR, \"roc_curves.png\"), dpi=140)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T15:15:41.085269Z","iopub.execute_input":"2025-10-09T15:15:41.086022Z","iopub.status.idle":"2025-10-09T15:15:41.481494Z","shell.execute_reply.started":"2025-10-09T15:15:41.085996Z","shell.execute_reply":"2025-10-09T15:15:41.480738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reliability / calibration: compare confidence vs accuracy\nconf = y_prob.max(axis=1)                 # predicted confidence\ncorrect = (y_pred == y_true).astype(int)  # 1/0 per sample\n\nbins = np.linspace(0.0, 1.0, 11)  # 10 bins\nbin_ids = np.digitize(conf, bins) - 1\nbin_acc, bin_conf, bin_count = [], [], []\n\nECE = 0.0\nN = len(conf)\nfor b in range(len(bins)-1):\n    idx = (bin_ids == b)\n    if np.any(idx):\n        acc_b = correct[idx].mean()\n        conf_b = conf[idx].mean()\n        cnt_b = idx.sum()\n        ECE += (cnt_b / N) * abs(acc_b - conf_b)\n        bin_acc.append(acc_b); bin_conf.append(conf_b); bin_count.append(cnt_b)\n    else:\n        bin_acc.append(np.nan); bin_conf.append(np.nan); bin_count.append(0)\n\n# Plot\ncenters = 0.5*(bins[:-1] + bins[1:])\nplt.figure(figsize=(5.2,5))\nplt.plot([0,1],[0,1],'k--',label='Perfect calibration')\nplt.scatter(centers, bin_acc, s=np.array(bin_count)*2+5, label='Observed acc')\nplt.plot(centers, bin_conf, label='Mean conf', alpha=0.7)\nplt.xlim(0,1); plt.ylim(0,1)\nplt.xlabel(\"Confidence\"); plt.ylabel(\"Accuracy\")\nplt.title(f\"Reliability Diagram (ECE={ECE:.3f})\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(os.path.join(OUT_DIR, \"reliability_diagram.png\"), dpi=140)\nplt.show()\n\nprint(f\"Expected Calibration Error (ECE): {ECE:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T15:15:46.361288Z","iopub.execute_input":"2025-10-09T15:15:46.361969Z","iopub.status.idle":"2025-10-09T15:15:46.696653Z","shell.execute_reply.started":"2025-10-09T15:15:46.361945Z","shell.execute_reply":"2025-10-09T15:15:46.695931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 0) Imports\n# =========================\nimport os, math, numpy as np, tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, losses, metrics\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\nfrom tensorflow.keras.mixed_precision import set_global_policy\n\n# Optional: mixed precision for speed on P100 (usually beneficial)\nset_global_policy(\"mixed_float16\")\n\n# Keras ConvNeXt (requires TF/Keras >= 2.11)\nfrom tensorflow.keras.applications import ConvNeXtTiny\nfrom tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\n\n# =========================\n# 1) Config\n# =========================\nIMG_SIZE = (224, 224)         # ConvNeXt default\nBATCH_SIZE = 32\nEPOCHS_WARMUP = 3             # freeze backbone first\nEPOCHS_FINETUNE = 12          # then unfreeze\nLR_WARMUP = 1e-3\nLR_FINETUNE = 5e-4\nAUTOTUNE = tf.data.AUTOTUNE\nN_FOLDS = len(folds_data)     # should be 5 for your setup\nN_CLASSES = len(class_names)\n\n# =========================\n# 2) Data pipeline (with MixUp/CutMix; ConvNeXt preprocess)\n# =========================\ndef decode_img_for_convnext(file_path, label):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)  # change to decode_png if needed\n    img = tf.image.resize(img, IMG_SIZE)\n    img = tf.cast(img, tf.float32)               # keep [0..255] for convnext_preprocess\n    return img, label\n\n# light augmentations\ndata_augment = tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.05),\n    layers.RandomContrast(0.1),\n], name=\"augment\")\n\ndef apply_preprocess(x, y):\n    x = convnext_preprocess(tf.cast(x, tf.float32))  # ConvNeXt normalization\n    return x, y\n\n# --- MixUp & CutMix (custom implementation that outputs one-hot labels) ---\ndef _sample_beta(alpha=0.2):\n    # Beta(alpha, alpha) via Gamma sampling (scalar)\n    g1 = tf.random.gamma(shape=[1], alpha=alpha)\n    g2 = tf.random.gamma(shape=[1], alpha=alpha)\n    lam = g1 / (g1 + g2)\n    return tf.cast(lam[0], tf.float32)\n\ndef _mixup_pair(x1, y1, x2, y2, alpha=0.2):\n    lam = _sample_beta(alpha)\n    x = lam * x1 + (1.0 - lam) * x2\n    y1 = tf.one_hot(y1, N_CLASSES)\n    y2 = tf.one_hot(y2, N_CLASSES)\n    y = lam * y1 + (1.0 - lam) * y2\n    return x, y\n\ndef _cutmix_pair(x1, y1, x2, y2, alpha=1.0):\n    lam = _sample_beta(alpha)\n    h, w = IMG_SIZE\n    cut_w = tf.cast(w * tf.sqrt(1.0 - lam), tf.int32)\n    cut_h = tf.cast(h * tf.sqrt(1.0 - lam), tf.int32)\n    # random center\n    cx = tf.random.uniform((), 0, w, dtype=tf.int32)\n    cy = tf.random.uniform((), 0, h, dtype=tf.int32)\n    x1_1 = tf.clip_by_value(cx - cut_w // 2, 0, w)\n    y1_1 = tf.clip_by_value(cy - cut_h // 2, 0, h)\n    x2_1 = tf.clip_by_value(cx + cut_w // 2, 0, w)\n    y2_1 = tf.clip_by_value(cy + cut_h // 2, 0, h)\n    # mask\n    pad_top, pad_bottom = y1_1, h - y2_1\n    pad_left, pad_right = x1_1, w - x2_1\n    mask = tf.pad(tf.ones((y2_1 - y1_1, x2_1 - x1_1, 3), dtype=tf.float32),\n                  [[pad_top, pad_bottom], [pad_left, pad_right], [0, 0]])\n    x = x1 * (1.0 - mask) + x2 * mask\n    lam_adj = 1.0 - tf.cast((x2_1 - x1_1) * (y2_1 - y1_1), tf.float32) / tf.cast(h * w, tf.float32)\n    y1 = tf.one_hot(y1, N_CLASSES)\n    y2 = tf.one_hot(y2, N_CLASSES)\n    y = lam_adj * y1 + (1.0 - lam_adj) * y2\n    return x, y\n\ndef _apply_mixup_cutmix_batch(x, y, p_mixup=0.5, p_cutmix=0.5):\n    # x: (B,H,W,3), y: (B,) int labels\n    B = tf.shape(x)[0]\n    # shuffle indices (pair with another sample)\n    idx = tf.random.shuffle(tf.range(B))\n    x2, y2 = tf.gather(x, idx), tf.gather(y, idx)\n\n    # decide which images use MixUp or CutMix\n    r = tf.random.uniform((B,), 0, 1)\n    use_mix = r < p_mixup\n    use_cut = (r >= p_mixup) & (r < (p_mixup + p_cutmix))\n    # default one-hot (no mix) for the rest\n    y_oh = tf.one_hot(y, N_CLASSES)\n\n    # vectorized map over batch (tf.map_fn to keep graph)\n    def _per_sample(args):\n        xi, yi, xj, yj, use_m, use_c = args\n        def do_mix():\n            return _mixup_pair(xi, yi, xj, yj, alpha=0.2)\n        def do_cut():\n            return _cutmix_pair(xi, yi, xj, yj, alpha=1.0)\n        def no_mix():\n            return xi, tf.one_hot(yi, N_CLASSES)\n        return tf.case(\n            [(use_m, do_mix), (use_c, do_cut)],\n            default=no_mix, exclusive=True\n        )\n    x_mixed, y_mixed = tf.map_fn(\n        _per_sample, (x, y, x2, y2, use_mix, use_cut),\n        fn_output_signature=(tf.float32, tf.float32)\n    )\n    return x_mixed, y_mixed\n\ndef make_tf_dataset_convnext(file_paths, labels, training=True):\n    ds = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n    if training:\n        ds = ds.shuffle(len(file_paths), reshuffle_each_iteration=True)\n\n    # decode + augment\n    ds = ds.map(decode_img_for_convnext, num_parallel_calls=AUTOTUNE)\n    if training:\n        ds = ds.map(lambda x, y: (data_augment(x, training=True), y), num_parallel_calls=AUTOTUNE)\n\n    # preprocess first (keeps numeric stability)\n    ds = ds.map(apply_preprocess, num_parallel_calls=AUTOTUNE)\n\n    # batch before mix to mix within-batch (important!)\n    ds = ds.batch(BATCH_SIZE)\n\n    if training:\n        # MixUp/CutMix on the batch, then continue\n        ds = ds.map(lambda x, y: _apply_mixup_cutmix_batch(x, y, 0.5, 0.5), num_parallel_calls=AUTOTUNE)\n    else:\n        # one-hot labels for validation to match CategoricalCrossentropy\n        ds = ds.map(lambda x, y: (x, tf.one_hot(y, N_CLASSES)), num_parallel_calls=AUTOTUNE)\n\n    ds = ds.prefetch(AUTOTUNE)\n    return ds\n\n# =========================\n# 3) ConvNeXt-Tiny model builder\n# =========================\ndef build_convnext_tiny(num_classes=N_CLASSES, image_size=IMG_SIZE, trainable_backbone=False):\n    # Backbone\n    backbone = ConvNeXtTiny(\n        include_top=False,\n        weights=\"imagenet\",\n        input_shape=(image_size[0], image_size[1], 3),\n        pooling=None\n    )\n    backbone.trainable = trainable_backbone  # warmup: False; finetune: True\n\n    inputs = layers.Input(shape=(image_size[0], image_size[1], 3))\n    x = backbone(inputs, training=False)\n\n    # Head (GAP + BN + Dropout + Dense)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)  # keep float32 for stability\n\n    model = models.Model(inputs, outputs, name=\"ConvNeXtTiny_Kvasir\")\n    return model\n\n# =========================\n# 4) Training utilities (fixed LR for warmup, Cosine for finetune)\n# =========================\ndef compile_model_warmup(model, lr):\n    opt = optimizers.Adam(learning_rate=lr)\n    model.compile(\n        optimizer=opt,\n        loss=losses.CategoricalCrossentropy(),  # using one-hot labels everywhere\n        metrics=[metrics.CategoricalAccuracy(name=\"acc\")]\n    )\n\ndef compile_model_cosine(model, initial_lr, epochs, steps_per_epoch):\n    cosine = optimizers.schedules.CosineDecay(\n        initial_learning_rate=initial_lr,\n        decay_steps=epochs * steps_per_epoch,\n        alpha=1e-6 / initial_lr  # final LR = initial_lr * alpha\n    )\n    opt = optimizers.Adam(learning_rate=cosine)\n    model.compile(\n        optimizer=opt,\n        loss=losses.CategoricalCrossentropy(),\n        metrics=[metrics.CategoricalAccuracy(name=\"acc\")]\n    )\n\ndef get_callbacks(fold_id):\n    os.makedirs(\"convnext_runs\", exist_ok=True)\n    return [\n        EarlyStopping(monitor=\"val_acc\", patience=6, mode=\"max\", restore_best_weights=True, verbose=1),\n        ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.5, patience=3, verbose=1, min_lr=1e-6),\n        ModelCheckpoint(\n            filepath=f\"convnext_runs/best_convnextT_fold{fold_id}.h5\",\n            monitor=\"val_acc\", mode=\"max\", save_best_only=True, verbose=1\n        ),\n        CSVLogger(f\"convnext_runs/training_log_fold{fold_id}.csv\", append=False)\n    ]\n\n# =========================\n# 5) 5-Fold training (warmup + finetune)\n# =========================\nfold_metrics = []\n\ndef get_callbacks(fold_id):\n    os.makedirs(\"convnext_runs\", exist_ok=True)\n    return [\n        EarlyStopping(monitor=\"val_acc\", patience=6, mode=\"max\", restore_best_weights=True, verbose=1),\n        ModelCheckpoint(\n            filepath=f\"convnext_runs/best_convnextT_fold{fold_id}.h5\",\n            monitor=\"val_acc\", mode=\"max\", save_best_only=True, verbose=1\n        ),\n        CSVLogger(f\"convnext_runs/training_log_fold{fold_id}.csv\", append=False)\n    ]\n\nfor fold_id in range(N_FOLDS):\n    print(f\"\\n================= FOLD {fold_id+1}/{N_FOLDS} =================\")\n\n    train_paths = folds_data[fold_id]['train_paths']\n    train_labels = folds_data[fold_id]['train_labels']\n    val_paths   = folds_data[fold_id]['val_paths']\n    val_labels  = folds_data[fold_id]['val_labels']\n\n    train_ds = make_tf_dataset_convnext(train_paths, train_labels, training=True)\n    val_ds   = make_tf_dataset_convnext(val_paths,   val_labels,   training=False)\n\n    # 5.1 Warmup: freeze backbone\n    model = build_convnext_tiny(trainable_backbone=False)\n    compile_model_warmup(model, lr=LR_WARMUP)\n    cb = get_callbacks(fold_id)\n\n    print(\"\\n---- Warmup (frozen backbone) ----\")\n    model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=EPOCHS_WARMUP,\n        callbacks=cb,\n        verbose=1\n    )\n\n    # 5.2 Fine-tune: unfreeze backbone + cosine annealing\n    model.get_layer(index=1).trainable = True\n    compile_model_cosine(model, initial_lr=LR_FINETUNE, epochs=EPOCHS_FINETUNE, steps_per_epoch=len(train_ds))\n    cb = get_callbacks(fold_id)\n\n    print(\"\\n---- Fine-tuning (unfrozen backbone, cosine annealing) ----\")\n    model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=EPOCHS_FINETUNE,\n        callbacks=cb,\n        verbose=1\n    )\n\n    # Evaluate best weights\n    eval_res = model.evaluate(val_ds, verbose=0)\n    fold_metrics.append({\"fold\": fold_id+1, \"val_loss\": float(eval_res[0]), \"val_acc\": float(eval_res[1])})\n    print(f\"FOLD {fold_id+1} → val_loss: {eval_res[0]:.4f} | val_acc: {eval_res[1]:.4f}\")\n\n# =========================\n# 6) Summary across folds\n# =========================\nval_accs  = [m[\"val_acc\"]  for m in fold_metrics]\nval_losses= [m[\"val_loss\"] for m in fold_metrics]\n\nprint(\"\\n========= 5-Fold Summary (ConvNeXt-T + MixUp/CutMix + CosineLR) =========\")\nfor m in fold_metrics:\n    print(f\"Fold {m['fold']}:  val_acc={m['val_acc']:.4f},  val_loss={m['val_loss']:.4f}\")\n\nprint(f\"\\nMean val_acc: {np.mean(val_accs):.4f}  (± {np.std(val_accs):.4f})\")\nprint(f\"Mean val_loss: {np.mean(val_losses):.4f} (± {np.std(val_losses):.4f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T13:29:13.400324Z","iopub.execute_input":"2025-10-10T13:29:13.400613Z","iopub.status.idle":"2025-10-10T15:04:42.433909Z","shell.execute_reply.started":"2025-10-10T13:29:13.400578Z","shell.execute_reply":"2025-10-10T15:04:42.433221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inline visualization of training/validation curves from CSVLogger outputs\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nfrom pathlib import Path\n\nLOG_DIR = Path(\"/kaggle/working/convnext_runs\")\nN_FOLDS = 5  # change if needed\n\ndef pick_cols(df):\n    m = {c.lower().strip(): c for c in df.columns}\n    acc = m.get(\"acc\") or m.get(\"accuracy\") or m.get(\"sparse_categorical_accuracy\") or m.get(\"categorical_accuracy\")\n    val_acc = m.get(\"val_acc\") or m.get(\"val_accuracy\") or m.get(\"val_sparse_categorical_accuracy\") or m.get(\"val_categorical_accuracy\")\n    loss = m.get(\"loss\")\n    val_loss = m.get(\"val_loss\")\n    return acc, val_acc, loss, val_loss\n\n# --- Per-fold panels ---\nfor f in range(N_FOLDS):\n    csvp = LOG_DIR / f\"training_log_fold{f}.csv\"\n    if not csvp.exists():\n        print(f\"[skip] {csvp} not found\"); \n        continue\n    df = pd.read_csv(csvp)\n    acc, val_acc, loss, val_loss = pick_cols(df)\n    epochs = np.arange(1, len(df)+1)\n\n    # Accuracy\n    plt.figure(figsize=(7,4))\n    if acc:     plt.plot(epochs, df[acc], label=\"train acc\")\n    if val_acc: plt.plot(epochs, df[val_acc], label=\"val acc\")\n    plt.title(f\"Fold {f} • Accuracy\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.grid(True, linewidth=0.3); plt.legend()\n    plt.show()\n\n    # Loss\n    plt.figure(figsize=(7,4))\n    if loss:     plt.plot(epochs, df[loss], label=\"train loss\")\n    if val_loss: plt.plot(epochs, df[val_loss], label=\"val loss\")\n    plt.title(f\"Fold {f} • Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.grid(True, linewidth=0.3); plt.legend()\n    plt.show()\n\n# --- All-folds comparison (validation curves only) ---\nplt.figure(figsize=(8,5))\nfor f in range(N_FOLDS):\n    csvp = LOG_DIR / f\"training_log_fold{f}.csv\"\n    if not csvp.exists(): continue\n    df = pd.read_csv(csvp)\n    _, val_acc, _, val_loss = pick_cols(df)\n    if val_acc: plt.plot(np.arange(1, len(df)+1), df[val_acc], label=f\"Fold {f}\")\nplt.title(\"Validation Accuracy — All Folds\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Accuracy\"); plt.grid(True, linewidth=0.3); plt.legend()\nplt.show()\n\nplt.figure(figsize=(8,5))\nfor f in range(N_FOLDS):\n    csvp = LOG_DIR / f\"training_log_fold{f}.csv\"\n    if not csvp.exists(): continue\n    df = pd.read_csv(csvp)\n    _, _, _, val_loss = pick_cols(df)\n    if val_loss: plt.plot(np.arange(1, len(df)+1), df[val_loss], label=f\"Fold {f}\")\nplt.title(\"Validation Loss — All Folds\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Loss\"); plt.grid(True, linewidth=0.3); plt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T15:31:52.450275Z","iopub.execute_input":"2025-10-10T15:31:52.450566Z","iopub.status.idle":"2025-10-10T15:31:54.554708Z","shell.execute_reply.started":"2025-10-10T15:31:52.450546Z","shell.execute_reply":"2025-10-10T15:31:54.553997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============== Robust Grad-CAM (manual forward; heatmap only) ===============\nimport os, glob, random, numpy as np, tensorflow as tf, cv2\nfrom pathlib import Path\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import ConvNeXtTiny\nfrom tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\n\n# --- CONFIG ---\nDATA_ROOT = \"/kaggle/input/kvasir-final-preprocessed-data/kvasir_bilateral_filtered\"\nWEIGHTS   = Path(\"/kaggle/working/convnext_runs/best_convnextT_fold4.h5\")  # change fold if you want\nIMG_SIZE  = (224, 224)\nN_CLASSES = 8\nTARGET    = \"pred\"  # \"pred\" or an int class id\nOUT_DIR   = Path(\"/kaggle/working/gradcam_heatmaps\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# --- Build SAME head you trained and load weights ---\ndef build_model(nc=N_CLASSES, image_size=IMG_SIZE):\n    bb = ConvNeXtTiny(include_top=False, weights=None,\n                      input_shape=(image_size[0], image_size[1], 3), pooling=None)\n    inp = layers.Input(shape=(image_size[0], image_size[1], 3))\n    x = bb(inp, training=False)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    out = layers.Dense(nc, activation=\"softmax\", dtype=\"float32\")(x)\n    return models.Model(inp, out)\n\nmodel = build_model()\nmodel.load_weights(str(WEIGHTS))\n\n# --- Pick a real image (or set IMG_PATH yourself) ---\ndef pick_image(root):\n    exts = (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\",\"*.JPEG\")\n    c = []\n    for e in exts: c += glob.glob(os.path.join(root, \"**\", e), recursive=True)\n    assert c, f\"No images under {root}\"\n    return random.choice(c)\n\nIMG_PATH = pick_image(DATA_ROOT)\nprint(\"Using image:\", IMG_PATH)\n\n# --- Preprocess ---\ndef load_for_model(fp, size):\n    img = tf.io.read_file(fp)\n    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n    img = tf.image.resize(img, size)\n    x = tf.cast(img, tf.float32)          # [0..255]\n    x = convnext_preprocess(x)            # ConvNeXt preprocessing\n    return tf.expand_dims(x, 0), tf.cast(img, tf.uint8)\n\nx, img_uint8 = load_for_model(IMG_PATH, IMG_SIZE)\n\n# --- Helpers ---\ndef find_last_4d_layer_name(m):\n    # walk from end to start to get the deepest 4D (HWC) tensor layer\n    for lyr in reversed(m.layers):\n        shp = getattr(lyr, \"output_shape\", None)\n        if isinstance(shp, tuple) and len(shp) == 4:\n            return lyr.name\n    raise ValueError(\"No 4D conv feature layer found for Grad-CAM.\")\n\nLAST_NAME = find_last_4d_layer_name(model)\n\ndef gradcam_manual(m, x, class_idx, last_name, img_size):\n    \"\"\"\n    Manual forward through layers inside one GradientTape to avoid Keras-3 multi-output issues.\n    \"\"\"\n    with tf.GradientTape() as tape:\n        t = x\n        conv_out = None\n        for lyr in m.layers[1:]:  # skip InputLayer at index 0\n            t = lyr(t, training=False)\n            if lyr.name == last_name:\n                conv_out = t\n        preds = t                            # (1, C)\n        target = preds[:, class_idx]         # scalar per sample\n    assert conv_out is not None, \"Last conv feature not found during forward pass.\"\n    grads   = tape.gradient(target, conv_out)          # (1, h, w, C)\n    weights = tf.reduce_mean(grads, axis=(0,1,2))      # (C,)\n    cam     = tf.tensordot(conv_out[0], weights, axes=1)  # (h, w)\n    cam     = tf.nn.relu(cam)\n    cam     = cam / (tf.reduce_max(cam) + 1e-12)\n    cam     = tf.image.resize(cam[...,None], img_size)[...,0]\n    return cam.numpy(), preds.numpy()[0]\n\n# --- First pass to get predicted class (then CAM for it) ---\ncam_tmp, probs0 = gradcam_manual(model, x, class_idx=0, last_name=LAST_NAME, img_size=IMG_SIZE)\npred_id = int(np.argmax(probs0))\nclass_idx = pred_id if TARGET == \"pred\" else int(TARGET)\n\n# --- Final CAM for chosen class ---\ncam, probs = gradcam_manual(model, x, class_idx=class_idx, last_name=LAST_NAME, img_size=IMG_SIZE)\n\n# --- Save grayscale heatmap ---\nout_path = OUT_DIR / f\"cam_{Path(IMG_PATH).stem}_class{class_idx}.png\"\ncv2.imwrite(str(out_path), (np.clip(cam,0,1)*255).astype(np.uint8))\nprint(f\"Saved heatmap → {out_path}\")\nprint(\"Predicted class id:\", pred_id, \"| CAM for class id:\", class_idx)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T15:33:12.730926Z","iopub.execute_input":"2025-10-10T15:33:12.731180Z","iopub.status.idle":"2025-10-10T15:33:28.920880Z","shell.execute_reply.started":"2025-10-10T15:33:12.731163Z","shell.execute_reply":"2025-10-10T15:33:28.920114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================  One-shot PostSegXAI-style dashboard  =====================\n# Produces a 2x3 figure: Original | Grad-CAM overlay | Uncertainty(1-CAM)\n#                        Class probabilities | Confidence regions | Text summary\nimport os, glob, json, random, numpy as np, tensorflow as tf, cv2, matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import ConvNeXtTiny\nfrom tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\n\n# ---------- CONFIG ----------\nDATA_ROOT  = \"/kaggle/input/kvasir-final-preprocessed-data/kvasir_bilateral_filtered\"\nWEIGHTS    = \"/kaggle/working/convnext_runs/best_convnextT_fold4.h5\"  # change fold if you want\nIMG_PATH   = None   # put a specific file path, or leave None and we'll auto-pick\nIMG_SIZE   = (224, 224)\nN_CLASSES  = 8      # set to your dataset\nALPHA      = 0.45   # overlay strength\n\n# Optional: class names (edit if you want pretty labels)\n# If you saved meta.json earlier, we will try to read from it automatically.\nCLASS_NAMES = [\"dyed-lifted-polyps\",\"dyed-resection-margins\",\"esophagitis\",\"normal-cecum\",\n               \"normal-pylorus\",\"normal-z-line\",\"polyps\",\"ulcerative\"]  # <-- change to your order if needed\n\n# ---------- Utilities ----------\ndef pick_image(root):\n    exts = (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\",\"*.JPEG\")\n    c = []\n    for e in exts: c += glob.glob(os.path.join(root, \"**\", e), recursive=True)\n    assert c, f\"No images under {root}\"\n    return random.choice(c)\n\nif IMG_PATH is None:\n    IMG_PATH = pick_image(DATA_ROOT)\n\n# Try reading class names from a nearby meta.json if present\nMETA_CAND = list(Path(\"/kaggle/working\").glob(\"convnext_artifacts_*/meta.json\"))\nif META_CAND:\n    try:\n        meta = json.load(open(META_CAND[-1]))\n        if \"class_names\" in meta and len(meta[\"class_names\"]) == N_CLASSES:\n            CLASS_NAMES = meta[\"class_names\"]\n    except Exception:\n        pass\n\n# True label (optional) from folder name in dataset\ntrue_label_name = Path(IMG_PATH).parent.name  # e.g., \"normal-z-line\" if folder name equals class name\ntrue_label_idx  = CLASS_NAMES.index(true_label_name) if true_label_name in CLASS_NAMES else None\n\n# Build SAME head you trained & load weights\ndef build_model(nc=N_CLASSES, image_size=IMG_SIZE):\n    bb = ConvNeXtTiny(include_top=False, weights=None,\n                      input_shape=(image_size[0], image_size[1], 3), pooling=None)\n    inp = layers.Input(shape=(image_size[0], image_size[1], 3))\n    x = bb(inp, training=False)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    out = layers.Dense(nc, activation=\"softmax\", dtype=\"float32\")(x)\n    return models.Model(inp, out)\n\nmodel = build_model()\nmodel.load_weights(WEIGHTS)\n\ndef load_for_model(fp, size):\n    img_bgr = cv2.imread(fp)  # BGR\n    if img_bgr is None:\n        # fallback via tf if OpenCV can't read\n        img = tf.io.read_file(fp)\n        img = tf.io.decode_image(img, channels=3, expand_animations=False).numpy()[:, :, ::-1]  # RGB\n    else:\n        img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, size, interpolation=cv2.INTER_AREA)\n    x = img.astype(np.float32)\n    x = convnext_preprocess(x)            # ConvNeXt preprocessing\n    return img.astype(np.uint8), np.expand_dims(x, 0)   # (H,W,3) uint8, (1,H,W,3) float\n\norig_rgb, x = load_for_model(IMG_PATH, IMG_SIZE)\n\n# ---- Robust Grad-CAM (manual forward; avoids Keras 3 multi-output quirks)\ndef find_last_4d_layer_name(m):\n    for lyr in reversed(m.layers):\n        shp = getattr(lyr, \"output_shape\", None)\n        if isinstance(shp, tuple) and len(shp) == 4:\n            return lyr.name\n    raise ValueError(\"No 4D conv feature layer found.\")\n\nLAST_NAME = find_last_4d_layer_name(model)\n\ndef gradcam_manual(m, x, class_idx, last_name, img_size):\n    with tf.GradientTape() as tape:\n        t = x\n        conv_out = None\n        for lyr in m.layers[1:]:  # skip InputLayer at 0\n            t = lyr(t, training=False)\n            if lyr.name == last_name:\n                conv_out = t\n        preds = t  # (1, C)\n        target = preds[:, class_idx]\n    assert conv_out is not None, \"Could not capture last conv output.\"\n    grads   = tape.gradient(target, conv_out)          # (1,h,w,C)\n    weights = tf.reduce_mean(grads, axis=(0,1,2))      # (C,)\n    cam     = tf.tensordot(conv_out[0], weights, axes=1)\n    cam     = tf.nn.relu(cam)\n    cam     = cam / (tf.reduce_max(cam) + 1e-12)\n    cam     = tf.image.resize(cam[...,None], img_size)[...,0]\n    return cam.numpy(), preds.numpy()[0]\n\n# First pass to get prediction\n_ , probs0 = gradcam_manual(model, x, class_idx=0, last_name=LAST_NAME, img_size=IMG_SIZE)\npred_idx = int(np.argmax(probs0))\npred_conf = float(np.max(probs0))\npred_name = CLASS_NAMES[pred_idx] if 0 <= pred_idx < len(CLASS_NAMES) else f\"class {pred_idx}\"\n\n# Final CAM for predicted class\ncam, probs = gradcam_manual(model, x, class_idx=pred_idx, last_name=LAST_NAME, img_size=IMG_SIZE)\n\n# -------- Visual parts --------\n# Grad-CAM overlay\nhm_u8  = (np.clip(cam, 0, 1) * 255).astype(np.uint8)\nhm_rgb = cv2.applyColorMap(hm_u8, cv2.COLORMAP_JET)[:, :, ::-1]   # RGB\noverlay = cv2.addWeighted(orig_rgb, 1.0, hm_rgb, ALPHA, 0)\n\n# “Uncertainty” map = (1 - CAM) for a quick visual proxy\nunc = (1.0 - np.clip(cam, 0, 1))\nunc_rgb = cv2.applyColorMap((unc * 255).astype(np.uint8), cv2.COLORMAP_TURBO)[:, :, ::-1]\n\n# Class probability bar chart\nfig = plt.figure(figsize=(14, 8))\ngs  = fig.add_gridspec(2, 3, height_ratios=[1,1.05], wspace=0.30, hspace=0.35)\n\n# (1) Original\nax1 = fig.add_subplot(gs[0,0])\nax1.imshow(orig_rgb); ax1.axis('off'); ax1.set_title(\"Original Image\")\n\n# (2) Grad-CAM overlay\nax2 = fig.add_subplot(gs[0,1])\nax2.imshow(overlay); ax2.axis('off'); ax2.set_title(f\"Grad-CAM\\nPred: {pred_name}\")\n\n# (3) Uncertainty map\nax3 = fig.add_subplot(gs[0,2])\nax3.imshow(unc_rgb); ax3.axis('off'); ax3.set_title(f\"Uncertainty Map\\nConfidence: {pred_conf*100:.2f}%\")\n\n# (4) Class probabilities\nax4 = fig.add_subplot(gs[1,0])\nclasses = CLASS_NAMES if len(CLASS_NAMES)==len(probs) else [str(i) for i in range(len(probs))]\nax4.bar(range(len(probs)), probs, tick_label=classes)\nax4.set_ylim(0,1.05)\nax4.set_title(\"Class Probabilities\")\nax4.tick_params(axis='x', labelrotation=35)\nax4.set_ylabel(\"Probability\")\n\n# (5) Confidence regions (column-wise CAM profile)\nax5 = fig.add_subplot(gs[1,1])\nprofile = cam.mean(axis=0)  # average over rows -> per-column confidence\nH, W = cam.shape\ncanvas = np.zeros((H, W, 3), dtype=np.uint8)\n# draw vertical lines with color by intensity (red->yellow->green)\nfor j, v in enumerate(profile):\n    if v < 0.05: \n        continue\n    color = (int(255*(1-v)), int(255*v), 0)  # R->G\n    cv2.line(canvas, (j, int(H*(1-v))), (j, H-1), color, 1)\nax5.imshow(canvas); ax5.axis('off'); ax5.set_title(\"Confidence Regions\")\n\n# (6) Text summary\nax6 = fig.add_subplot(gs[1,2]); ax6.axis('off')\ntext_lines = [\n    \"Prediction Summary:\",\n    f\"Class: {pred_name}\",\n    f\"Confidence: {pred_conf*100:.2f}%\",\n]\nif true_label_idx is not None:\n    corr = \"✓\" if pred_idx == true_label_idx else \"✗\"\n    text_lines += [f\"True Label: {CLASS_NAMES[true_label_idx]}\", f\"Correct: {corr}\"]\nax6.text(0.02, 0.98, \"\\n\".join(text_lines), va='top', fontsize=12)\n\nplt.show()\nprint(\"Done. (Image:\", IMG_PATH, \")\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T15:33:38.876380Z","iopub.execute_input":"2025-10-10T15:33:38.876679Z","iopub.status.idle":"2025-10-10T15:33:48.084249Z","shell.execute_reply.started":"2025-10-10T15:33:38.876654Z","shell.execute_reply":"2025-10-10T15:33:48.083617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, re\n\n# find any best_convnextT_fold*.h5 file\nckpts = [f for f in os.listdir(\"convnext_runs\") if re.match(r\"best_convnextT_fold\\d+\\.h5$\", f)]\nassert ckpts, \"No .h5 checkpoints found in convnext_runs/\"\n# pick the highest-numbered fold (or change logic as you like)\nckpts_sorted = sorted(ckpts, key=lambda s: int(re.search(r\"fold(\\d+)\", s).group(1)))\nckpt = ckpts_sorted[-1]\nFOLD_ID = int(re.search(r\"fold(\\d+)\", ckpt).group(1))\nprint(\"Loading checkpoint:\", ckpt, \"| FOLD_ID =\", FOLD_ID)\n\nmodel = build_convnext_tiny()\nmodel.load_weights(os.path.join(\"convnext_runs\", ckpt))\nprint(\"Model rebuilt + weights loaded.\")\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T15:36:27.382226Z","iopub.execute_input":"2025-10-10T15:36:27.382992Z","iopub.status.idle":"2025-10-10T15:36:28.702417Z","shell.execute_reply.started":"2025-10-10T15:36:27.382967Z","shell.execute_reply":"2025-10-10T15:36:28.701795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths for this fold\nval_paths = folds_data[FOLD_ID]['val_paths']\nval_labels = folds_data[FOLD_ID]['val_labels']\n\ndef load_img_for_convnext(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)    # change to decode_png if needed\n    img = tf.image.resize(img, IMG_SIZE)\n    img = tf.cast(img, tf.float32)                 # [0..255]\n    return img\n\ndef predict_softmax(img_raw):\n    x = convnext_preprocess(img_raw[None, ...])    # preprocess inside\n    p = model(x, training=False).numpy()[0]        # (C,)\n    pred = int(np.argmax(p))\n    conf = float(np.max(p))\n    # uncertainty via entropy (nats)\n    entropy = float(-np.sum(p * np.log(p + 1e-12)))\n    return pred, conf, entropy, p\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T15:37:04.560493Z","iopub.execute_input":"2025-10-10T15:37:04.561311Z","iopub.status.idle":"2025-10-10T15:37:04.566819Z","shell.execute_reply.started":"2025-10-10T15:37:04.561285Z","shell.execute_reply":"2025-10-10T15:37:04.566039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find last 4D feature layer automatically (should be 'convnext_tiny')\ndef get_last_4d_layer(m):\n    for lyr in reversed(m.layers):\n        try:\n            shp = lyr.output_shape\n        except:\n            continue\n        if isinstance(shp, tuple) and len(shp) == 4:\n            return lyr.name\n    return m.layers[1].name  # fallback\n\nLAST_FEAT = get_last_4d_layer(model)\nprint(\"Grad-CAM feature layer:\", LAST_FEAT)\n\n@tf.function\ndef _gradcam_core(img_batch, target_idx, feat_model, head_model):\n    with tf.GradientTape() as tape:\n        feats = feat_model(img_batch)                       # (B,h,w,c)\n        tape.watch(feats)\n        logits = head_model(feats, training=False)          # (B,C)\n        cls = logits[:, target_idx]\n    grads = tape.gradient(cls, feats)                       # (B,h,w,c)\n    w = tf.reduce_mean(grads, axis=(1,2), keepdims=True)    # (B,1,1,c)\n    cam = tf.nn.relu(tf.reduce_sum(w * feats, axis=-1))     # (B,h,w)\n    # normalize per-image\n    cam_min = tf.reduce_min(cam, axis=(1,2), keepdims=True)\n    cam_max = tf.reduce_max(cam, axis=(1,2), keepdims=True)\n    cam = (cam - cam_min) / (cam_max - cam_min + 1e-8)\n    return cam\n\n# Split model into feature extractor + head (after LAST_FEAT)\ndef split_feature_and_head(m, last_feat_name=LAST_FEAT):\n    feat_layer = m.get_layer(last_feat_name)\n    feat_model = tf.keras.Model(m.input, feat_layer.output)              # input -> conv feat\n    # build head: from conv feat to logits, reusing layers after feat layer\n    idx = [i for (i, l) in enumerate(m.layers) if l.name == last_feat_name][0]\n    x = tf.keras.Input(shape=feat_layer.output_shape[1:])\n    y = x\n    for lyr in m.layers[idx+1:]:\n        y = lyr(y)\n    head_model = tf.keras.Model(x, y)\n    return feat_model, head_model\n\nfeat_model, head_model = split_feature_and_head(model, LAST_FEAT)\n\ndef gradcam_heatmap(img_raw, target_idx):\n    x = convnext_preprocess(img_raw[None, ...])\n    cam_small = _gradcam_core(x, tf.constant(target_idx, tf.int32), feat_model, head_model)[0] # (h,w)\n    cam = tf.image.resize(cam_small[..., None], IMG_SIZE)[...,0].numpy()\n    return np.clip(cam, 0, 1)\n\ndef overlay_heatmap(img_raw, cam, alpha=0.45):\n    img = (img_raw.numpy() / 255.0)\n    cmap = plt.get_cmap('jet')\n    heat = cmap(cam)[..., :3]\n    out = (1 - alpha) * img + alpha * heat\n    return np.clip(out, 0, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T15:37:07.843694Z","iopub.execute_input":"2025-10-10T15:37:07.844270Z","iopub.status.idle":"2025-10-10T15:37:07.867986Z","shell.execute_reply.started":"2025-10-10T15:37:07.844246Z","shell.execute_reply":"2025-10-10T15:37:07.867236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Backbone (ConvNeXt) as feature extractor\nbackbone = model.get_layer(\"convnext_tiny\")\n\nx_in = tf.keras.Input(shape=(224, 224, 3))     # plain input tensor\nfeat_out = backbone(x_in)                      # (None, 7, 7, 768)\nfeat_model = tf.keras.Model(x_in, feat_out, name=\"feat_model\")\n\n# Head model: GAP -> BN -> Dropout -> Dense\nf_in = tf.keras.Input(shape=backbone.output.shape[1:])  # (7, 7, 768)\ngap  = model.get_layer(\"global_average_pooling2d_15\")(f_in)\nbn   = model.get_layer(\"batch_normalization_15\")(gap)\ndrop = model.get_layer(\"dropout_15\")(bn)\nout  = model.get_layer(\"dense_15\")(drop)\nhead_model = tf.keras.Model(f_in, out, name=\"head_model\")\n\nprint(\"✅ Rebuilt feat_model and head_model with fresh inputs\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T15:37:31.737353Z","iopub.execute_input":"2025-10-10T15:37:31.737659Z","iopub.status.idle":"2025-10-10T15:37:31.750908Z","shell.execute_reply.started":"2025-10-10T15:37:31.737637Z","shell.execute_reply":"2025-10-10T15:37:31.750205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _gradcam_core(img_batch, target_idx, feat_model, head_model):\n    with tf.GradientTape() as tape:\n        feats = feat_model(img_batch, training=False)   # (B, 7, 7, 768)\n        tape.watch(feats)\n        logits = head_model(feats, training=False)      # (B, num_classes)\n        loss = logits[:, target_idx]\n\n    grads = tape.gradient(loss, feats)                  # (B,7,7,768)\n    pooled = tf.reduce_mean(grads, axis=(1,2), keepdims=True)\n    cam = tf.reduce_sum(feats * pooled, axis=-1)        # (B,7,7)\n    return cam\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T15:37:35.425507Z","iopub.execute_input":"2025-10-10T15:37:35.426216Z","iopub.status.idle":"2025-10-10T15:37:35.430693Z","shell.execute_reply.started":"2025-10-10T15:37:35.426192Z","shell.execute_reply":"2025-10-10T15:37:35.430051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def gradcam_heatmap(img_raw, target_idx):\n    x = convnext_preprocess(img_raw[None, ...])   # preprocess like training\n    cam_small = _gradcam_core(x, target_idx, feat_model, head_model)[0]\n    cam = tf.image.resize(cam_small[..., None], IMG_SIZE)[..., 0].numpy()\n    return np.maximum(cam, 0) / (cam.max() + 1e-8)   # normalize\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T15:37:38.360236Z","iopub.execute_input":"2025-10-10T15:37:38.361125Z","iopub.status.idle":"2025-10-10T15:37:38.366014Z","shell.execute_reply.started":"2025-10-10T15:37:38.361093Z","shell.execute_reply":"2025-10-10T15:37:38.365274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def overlay_heatmap(img_raw, cam, alpha=0.4):\n    heatmap = plt.cm.jet(cam)[..., :3] * 255.0\n    overlay = (1 - alpha) * img_raw + alpha * heatmap\n    return np.clip(overlay, 0, 255).astype(\"uint8\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T15:37:40.726070Z","iopub.execute_input":"2025-10-10T15:37:40.726374Z","iopub.status.idle":"2025-10-10T15:37:40.731383Z","shell.execute_reply.started":"2025-10-10T15:37:40.726352Z","shell.execute_reply":"2025-10-10T15:37:40.730485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== One-shot Grad-CAM overlay (self-contained) ====\n\nimport numpy as np, tensorflow as tf, matplotlib.pyplot as plt\nfrom tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\n\n# ----- 1) Pick a validation sample\nFOLD_ID = 4  # change if needed\nval_paths  = folds_data[FOLD_ID]['val_paths']\nval_labels = folds_data[FOLD_ID]['val_labels']\n\nidx = 0\npath = val_paths[idx]\ntrue_id = int(val_labels[idx])\ntrue_name = class_names[true_id]\n\n# ----- 2) Load raw resized image (float32 [0..255])\ndef load_img_for_convnext(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)  # switch to decode_png if needed\n    img = tf.image.resize(img, IMG_SIZE)\n    return tf.cast(img, tf.float32)\n\nimg_raw = load_img_for_convnext(path)\n\n# ----- 3) Predict softmax\ndef predict_softmax(img_raw_tf):\n    x = convnext_preprocess(img_raw_tf[None, ...])\n    p = model(x, training=False).numpy()[0]\n    pred_id = int(np.argmax(p))\n    conf = float(np.max(p))\n    ent  = float(-np.sum(p * np.log(p + 1e-12)))\n    return pred_id, conf, ent, p\n\npred_id, conf, ent, p = predict_softmax(img_raw)\n\n# ----- 4) Build feature & head models (fresh Inputs to avoid graph KeyErrors)\n# find last 4D feature layer (ConvNeXt backbone)\ndef get_last_4d_layer(m):\n    for lyr in reversed(m.layers):\n        try:\n            shp = lyr.output_shape\n            if isinstance(shp, tuple) and len(shp) == 4:\n                return lyr.name\n        except:\n            pass\n    raise RuntimeError(\"No 4D feature layer found.\")\nLAST_FEAT = get_last_4d_layer(model)  # usually 'convnext_tiny'\n\n# feature extractor: raw -> preprocess -> backbone features\nx_in = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\nx_proc = convnext_preprocess(x_in)\nfeat_out = model.get_layer(LAST_FEAT)(x_proc)\nfeat_model = tf.keras.Model(x_in, feat_out, name=\"feat_model_gc\")\n\n# head: replay layers after LAST_FEAT\nidx_last = [i for i,l in enumerate(model.layers) if l.name == LAST_FEAT][0]\nf_in = tf.keras.Input(shape=feat_out.shape[1:])\ny = f_in\nfor lyr in model.layers[idx_last+1:]:\n    y = lyr(y)\nhead_model = tf.keras.Model(f_in, y, name=\"head_model_gc\")\n\n# ----- 5) Grad-CAM\ndef gradcam_heatmap(img_raw_tf, target_idx):\n    with tf.GradientTape() as tape:\n        feats = feat_model(img_raw_tf[None, ...], training=False)   # (1,h,w,c)\n        tape.watch(feats)\n        logits = head_model(feats, training=False)                  # (1,C)\n        loss = logits[:, int(target_idx)]\n    grads = tape.gradient(loss, feats)                               # (1,h,w,c)\n    w = tf.reduce_mean(grads, axis=(1,2), keepdims=True)             # (1,1,1,c)\n    cam_small = tf.nn.relu(tf.reduce_sum(w * feats, axis=-1))        # (1,h,w)\n    cam = tf.image.resize(cam_small[..., None], IMG_SIZE)[0, ..., 0].numpy()\n    cam = np.maximum(cam, 0) / (cam.max() + 1e-8)\n    return cam\n\ncam = gradcam_heatmap(img_raw, pred_id)\n\n# ----- 6) Overlay utility\ndef overlay_heatmap(img_raw_tf, cam, alpha=0.45):\n    img = (img_raw_tf.numpy() / 255.0)  # (H,W,3) in [0,1]\n    heat = plt.get_cmap(\"jet\")(cam)[..., :3]\n    out = (1 - alpha) * img + alpha * heat\n    return np.clip(out * 255.0, 0, 255).astype(\"uint8\")\n\noverlay = overlay_heatmap(img_raw, cam, alpha=0.45)\n\n# ----- 7) Plot\nplt.figure(figsize=(10,5))\n\nplt.subplot(1,2,1)\nplt.imshow(img_raw.numpy().astype(\"uint8\"))\nplt.title(f\"Raw | GT={true_name} | Pred={class_names[pred_id]} ({conf:.2f})\")\nplt.axis(\"off\")\n\nplt.subplot(1,2,2)\nplt.imshow(overlay)\nplt.title(\"Grad-CAM Overlay\")\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T15:37:44.268227Z","iopub.execute_input":"2025-10-10T15:37:44.268849Z","iopub.status.idle":"2025-10-10T15:37:50.329137Z","shell.execute_reply.started":"2025-10-10T15:37:44.268826Z","shell.execute_reply":"2025-10-10T15:37:50.328405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Metrics & plotting imports + safe setup ---\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Ensure output dir exists\nOUT_DIR = OUT_DIR if 'OUT_DIR' in globals() else \"postsegxai_metrics\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# Sanity: N_CLASSES known\nassert 'N_CLASSES' in globals(), \"N_CLASSES not defined.\"\nassert 'class_names' in globals(), \"class_names not defined.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T15:38:01.326236Z","iopub.execute_input":"2025-10-10T15:38:01.326508Z","iopub.status.idle":"2025-10-10T15:38:01.331571Z","shell.execute_reply.started":"2025-10-10T15:38:01.326488Z","shell.execute_reply":"2025-10-10T15:38:01.330934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict entire validation fold\ny_true, y_pred = [], []\ny_prob = np.zeros((len(val_paths), N_CLASSES), dtype=np.float32)\n\nfor i, path in enumerate(val_paths):\n    img_raw = load_img_for_convnext(path)\n    pred, conf, ent, p = predict_softmax(img_raw)\n    y_true.append(val_labels[i])\n    y_pred.append(pred)\n    y_prob[i] = p\n\ny_true = np.array(y_true, int)\ny_pred = np.array(y_pred, int)\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred, labels=list(range(N_CLASSES)))\ncm_norm = cm / cm.sum(axis=1, keepdims=True)\n\nplt.figure(figsize=(6,5))\nplt.imshow(cm_norm, vmin=0, vmax=1)\nplt.title(\"Normalized Confusion Matrix (Val)\")\nplt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\nplt.xticks(range(N_CLASSES), class_names, rotation=45, ha='right')\nplt.yticks(range(N_CLASSES), class_names)\nfor i in range(N_CLASSES):\n    for j in range(N_CLASSES):\n        plt.text(j, i, f\"{cm_norm[i,j]:.2f}\", ha='center', va='center', fontsize=8, color='white' if cm_norm[i,j]>0.5 else 'black')\nplt.tight_layout()\nplt.savefig(os.path.join(OUT_DIR, \"confusion_matrix.png\"), dpi=140)\nplt.show()\n\n# Classification report (per-class precision/recall/F1)\nprint(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T15:38:04.710284Z","iopub.execute_input":"2025-10-10T15:38:04.710766Z","iopub.status.idle":"2025-10-10T16:14:29.688850Z","shell.execute_reply.started":"2025-10-10T15:38:04.710745Z","shell.execute_reply":"2025-10-10T16:14:29.688206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- ROC / AUC (one-vs-rest, with guards) ---\nimport os, numpy as np, matplotlib.pyplot as plt\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\n\n# Ensure OUT_DIR exists\nOUT_DIR = OUT_DIR if 'OUT_DIR' in globals() else \"postsegxai_metrics\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# Binarize labels for ROC\nY_true_bin = label_binarize(y_true, classes=list(range(N_CLASSES)))  # (N, C)\n\nfpr, tpr, roc_auc = {}, {}, {}\nvalid_classes = []\n\n# Per-class ROC (skip classes that are constant in this fold)\nfor c in range(N_CLASSES):\n    y_c = Y_true_bin[:, c]\n    # roc_curve needs both 0 and 1 present\n    if y_c.max() == y_c.min():\n        # no positives or no negatives for this class in the fold\n        continue\n    fpr[c], tpr[c], _ = roc_curve(y_c, y_prob[:, c])\n    roc_auc[c] = auc(fpr[c], tpr[c])\n    valid_classes.append(c)\n\n# Micro ROC (only if there is at least one valid class)\nif len(valid_classes) > 0:\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_true_bin[:, valid_classes].ravel(),\n                                              y_prob[:, valid_classes].ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Macro ROC over valid classes\nif len(valid_classes) > 0:\n    all_fpr = np.unique(np.concatenate([fpr[c] for c in valid_classes]))\n    mean_tpr = np.zeros_like(all_fpr)\n    for c in valid_classes:\n        mean_tpr += np.interp(all_fpr, fpr[c], tpr[c])\n    mean_tpr /= len(valid_classes)\n    roc_auc[\"macro\"] = auc(all_fpr, mean_tpr)\nelse:\n    all_fpr, mean_tpr = np.array([0,1]), np.array([0,1])\n    roc_auc[\"macro\"] = np.nan\n\n# Plot\nplt.figure(figsize=(6.5, 5.5))\nfor c in valid_classes:\n    plt.plot(fpr[c], tpr[c], lw=1, label=f\"{class_names[c]} (AUC={roc_auc[c]:.3f})\")\nif \"micro\" in fpr:\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"], lw=2, linestyle=\"--\",\n             label=f\"micro (AUC={roc_auc['micro']:.3f})\")\nplt.plot([0,1],[0,1], \"k--\", lw=1)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(f\"ROC (Val)  |  macro AUC={roc_auc['macro']:.3f}\" if not np.isnan(roc_auc[\"macro\"]) else \"ROC (Val)\")\nplt.legend(fontsize=7, loc=\"lower right\")\nplt.tight_layout()\nplt.savefig(os.path.join(OUT_DIR, \"roc_curves.png\"), dpi=140)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T16:35:06.120668Z","iopub.execute_input":"2025-10-10T16:35:06.120944Z","iopub.status.idle":"2025-10-10T16:35:06.561324Z","shell.execute_reply.started":"2025-10-10T16:35:06.120924Z","shell.execute_reply":"2025-10-10T16:35:06.560547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reliability / calibration: compare confidence vs accuracy\nconf = y_prob.max(axis=1)                 # predicted confidence\ncorrect = (y_pred == y_true).astype(int)  # 1/0 per sample\n\nbins = np.linspace(0.0, 1.0, 11)  # 10 bins\nbin_ids = np.digitize(conf, bins) - 1\nbin_acc, bin_conf, bin_count = [], [], []\n\nECE = 0.0\nN = len(conf)\nfor b in range(len(bins)-1):\n    idx = (bin_ids == b)\n    if np.any(idx):\n        acc_b = correct[idx].mean()\n        conf_b = conf[idx].mean()\n        cnt_b = idx.sum()\n        ECE += (cnt_b / N) * abs(acc_b - conf_b)\n        bin_acc.append(acc_b); bin_conf.append(conf_b); bin_count.append(cnt_b)\n    else:\n        bin_acc.append(np.nan); bin_conf.append(np.nan); bin_count.append(0)\n\n# Plot\ncenters = 0.5*(bins[:-1] + bins[1:])\nplt.figure(figsize=(5.2,5))\nplt.plot([0,1],[0,1],'k--',label='Perfect calibration')\nplt.scatter(centers, bin_acc, s=np.array(bin_count)*2+5, label='Observed acc')\nplt.plot(centers, bin_conf, label='Mean conf', alpha=0.7)\nplt.xlim(0,1); plt.ylim(0,1)\nplt.xlabel(\"Confidence\"); plt.ylabel(\"Accuracy\")\nplt.title(f\"Reliability Diagram (ECE={ECE:.3f})\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(os.path.join(OUT_DIR, \"reliability_diagram.png\"), dpi=140)\nplt.show()\n\nprint(f\"Expected Calibration Error (ECE): {ECE:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T16:35:12.720977Z","iopub.execute_input":"2025-10-10T16:35:12.721730Z","iopub.status.idle":"2025-10-10T16:35:13.075930Z","shell.execute_reply.started":"2025-10-10T16:35:12.721707Z","shell.execute_reply":"2025-10-10T16:35:13.075170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}